#!/bin/bash

"exec" "python" "-u" "-Wignore" "$0" "$@"

import os
import re
import sys
import time
import atexit
import commands
import optparse
import shelve
import datetime
import urllib
import random
import fcntl
import types
import traceback
import pickle

####################################################################

# error code
EC_Config    = 10
EC_CMT       = 20
EC_Extractor = 30
EC_Dataset   = 40
EC_Post      = 50
EC_Archive   = 60
EC_Split     = 70
EC_MyProxy   = 80

#@ Number of events to skip in the file
nEventsToSkip=0
#@ Events blok counter per file
nSkips =0

# default cloud/site
defaultCloud = 'US'

# suffix for shadow dataset
suffixShadow = "_shadow"


# max size per job
maxTotalSize = long(5*1024*1024*1024)
        

usage = """%prog [options] <jobOption1.py> [<jobOption2.py> [...]]

'%prog --help' prints a summary of the options"""


# command-line parameters
optP = optparse.OptionParser(usage=usage,conflict_handler="resolve")
# special options
optP.add_option('--version',action='store_const',const=True,dest='version',default=False,
                help='Displays version')
optP.add_option('--split', action='store', dest='split',  default=-1,
                type='int',    help='Number of sub-jobs to which a job is split')
optP.add_option('--nFilesPerJob', action='store', dest='nFilesPerJob',  default=-1, type='int', help='Number of files on which each sub-job runs')
optP.add_option('--nEventsPerJob', action='store', dest='nEventsPerJob',  default=-1,
                type='int',    help='Number of events on which each sub-job runs')
optP.add_option('--nEventsPerFile', action='store', dest='nEventsPerFile',  default=0,
                type='int',    help='Number of events per file')
optP.add_option('--site', action='store', dest='site',  default="AUTO",
                type='string',    help='Site name where jobs are sent (default:AUTO)')
optP.add_option('--inDS',  action='store', dest='inDS',  default='',
                type='string', help='Name of an input dataset')
optP.add_option('--minDS',  action='store', dest='minDS',  default='',
                type='string', help='Dataset name for minimum bias stream')
optP.add_option('--nMin',  action='store', dest='nMin',  default=-1,
                type='int', help='Number of minimum bias files per one signal file')
optP.add_option('--cavDS',  action='store', dest='cavDS',  default='',
                type='string', help='Dataset name for cavern stream')
optP.add_option('--nCav',  action='store', dest='nCav',  default=-1,
                type='int', help='Number of cavern files per one signal file')
optP.add_option('--libDS', action='store', dest='libDS', default='',
                type='string', help='Name of a library dataset')
optP.add_option('--useCommonHalo', action='store_const', const=False, dest='useCommonHalo',  default=True,
                help="use an integrated DS for BeamHalo")
optP.add_option('--beamHaloDS',  action='store', dest='beamHaloDS',  default='',
                type='string', help='Dataset name for beam halo')
optP.add_option('--beamHaloADS',  action='store', dest='beamHaloADS',  default='',
                type='string', help='Dataset name for beam halo A-side')
optP.add_option('--beamHaloCDS',  action='store', dest='beamHaloCDS',  default='',
                type='string', help='Dataset name for beam halo C-side')
optP.add_option('--nBeamHalo',  action='store', dest='nBeamHalo',  default=-1,
                type='int', help='Number of beam halo files per sub job')
optP.add_option('--nBeamHaloA',  action='store', dest='nBeamHaloA',  default=-1,
                type='int', help='Number of beam halo files for A-side per sub job')
optP.add_option('--nBeamHaloC',  action='store', dest='nBeamHaloC',  default=-1,
                type='int', help='Number of beam halo files for C-side per sub job')
optP.add_option('--useCommonGas', action='store_const', const=False, dest='useCommonGas',  default=True,
                help="use an integrated DS for BeamGas")
optP.add_option('--beamGasDS',  action='store', dest='beamGasDS',  default='',
                type='string', help='Dataset name for beam gas')
optP.add_option('--beamGasHDS',  action='store', dest='beamGasHDS',  default='',
                type='string', help='Dataset name for beam gas Hydrogen')
optP.add_option('--beamGasCDS',  action='store', dest='beamGasCDS',  default='',
                type='string', help='Dataset name for beam gas Carbon')
optP.add_option('--beamGasODS',  action='store', dest='beamGasODS',  default='',
                type='string', help='Dataset name for beam gas Oxygen')
optP.add_option('--nBeamGas',  action='store', dest='nBeamGas',  default=-1,
                type='int', help='Number of beam gas files per sub job')
optP.add_option('--nBeamGasH',  action='store', dest='nBeamGasH',  default=-1,
                type='int', help='Number of beam gas files for Hydrogen per sub job')
optP.add_option('--nBeamGasC',  action='store', dest='nBeamGasC',  default=-1,
                type='int', help='Number of beam gas files for Carbon per sub job')
optP.add_option('--nBeamGasO',  action='store', dest='nBeamGasO',  default=-1,
                type='int', help='Number of beam gas files for Oxygen per sub job')
optP.add_option('--outDS', action='store', dest='outDS', default='',
                type='string', help='Name of an output dataset. OUTDS will contain all output files')
optP.add_option('--destSE',action='store', dest='destSE',default='',
                type='string', help='Destination strorage element. All outputs go to DESTSE (default :%BNL_ATLAS_2)')
optP.add_option('--nFiles', '--nfiles', action='store', dest='nfiles',  default=0,
                type='int',    help='Use an limited number of files in the input dataset')
optP.add_option('--nSkipFiles', action='store', dest='nSkipFiles',  default=0,
                type='int',    help='Skip N files in the input dataset')
optP.add_option('-v', action='store_const', const=True, dest='verbose',  default=False,
                help='Verbose')
optP.add_option('-l', '--long', action='store_const', const=True, dest='long',  default=False,
                help='Send job to a long queue')
optP.add_option('--blong', action='store_const', const=True, dest='blong',  default=False,
                help='Send build job to a long queue')
optP.add_option('--update', action='store_const', const=True, dest='update',  default=False,
                help='Update panda-client to the latest version')
optP.add_option('--cloud',action='store', dest='cloud',default=None,
                type='string', help='cloud where jobs are submitted. default is set according to your VOMS country group')
optP.add_option('--noBuild', action='store_const', const=True, dest='nobuild',  default=False,
                help='Skip buildJob')
optP.add_option('--individualOutDS', action='store_const', const=True, dest='individualOutDS',  default=False,
                help='Create individual output dataset for each data-type. By default, all output files are added to one output dataset')
optP.add_option('--noRandom', action='store_const', const=True, dest='norandom',  default=False,
                help='Enter random seeds manually')
optP.add_option('--memory', action='store', dest='memory',  default=-1,
                type='int',    help='Required memory size')
optP.add_option('--maxCpuCount', action='store', dest='maxCpuCount', default=-1, type='int',
                help='Required CPU count in seconds. Mainly to extend time limit for looping detection')
optP.add_option('--official', action='store_const', const=True, dest='official',  default=False,
                help='Produce official dataset')
optP.add_option('--extFile', action='store', dest='extFile',  default='',
                help='pathena exports files with some special extensions (.C, .dat, .py .xml) in the current directory. If you want to add other files, specify their names, e.g., data1,root,data2.doc')
optP.add_option('--extOutFile', action='store', dest='extOutFile',  default='',
                help='define extra output files, e.g., output1.txt,output2.dat')
optP.add_option('--supStream', action='store', dest='supStream',  default='',
                help='suppress some output streams. e.g., ESD,TAG ')
optP.add_option('--excludedSite', action='store', dest='excludedSite',  default='',
                help="list of sites which are not used for site section, e.g., ANALY_ABC,ANALY_XYZ")
optP.add_option('--noSubmit', action='store_const', const=True, dest='nosubmit',  default=False,
                help="Don't submit jobs")
optP.add_option('--test', action='store_const', const=True, dest='testMode',  default=False,
                help="Submit test jobs")
optP.add_option('--generalInput', action='store_const', const=True, dest='generalInput',  default=False,
                help='Read input files with general format except POOL,ROOT,ByteStream')
optP.add_option('--crossSite',action='store',dest='crossSite',default=3,
                type='int',help='submit jobs to N sites at most when datasets in container split over many sites (N=3 by default)')
optP.add_option('--tmpDir', action='store', dest='tmpDir', default='',
                type='string', help='Temporary directory in which an archive file is created')
optP.add_option('--shipInput', action='store_const', const=True, dest='shipinput',  default=False,
                help='Ship input files to remote WNs')
optP.add_option('--noLock', action='store_const', const=True, dest='nolock',  default=False,
                help="Don't create a lock for local database access")
optP.add_option('--fileList', action='store', dest='filelist', default='',
                type='string', help='List of files in the input dataset to be run')
optP.add_option('--myproxy', action='store', dest='myproxy', default='pandaprx.usatlas.bnl.gov',
                type='string', help='Name of the myproxy server')
optP.add_option('--dbRelease', action='store', dest='dbRelease', default='',
                type='string', help='DBRelease or CDRelease (DatasetName:FileName). e.g., ddo.000001.Atlas.Ideal.DBRelease.v050101:DBRelease-5.1.1.tar.gz')
optP.add_option('--dbRunNumber', action='store', dest='dbRunNumber', default='',
                type='string', help='RunNumber for DBRelease or CDRelease. If this option is used some redundant files are removed to save disk usage when unpacking DBRelease tarball. e.g., 0091890')
optP.add_option('--addPoolFC', action='store', dest='addPoolFC',  default='',
                help="file names to be inserted into PoolFileCatalog.xml except input files. e.g., MyCalib1.root,MyGeom2.root") 
optP.add_option('--skipScan', action='store_const', const=True, dest='skipScan', default=False,
                help='Skip LRC/LFC lookup at job submission')
optP.add_option('--inputFileList', action='store', dest='inputFileList', default='',
                type='string', help='name of file which contains a list of files to be run in the input dataset')
optP.add_option('--removeFileList', action='store', dest='removeFileList', default='',
                type='string', help='name of file which contains a list of files to be removed from the input dataset')
optP.add_option('--corCheck', action='store_const', const=True, dest='corCheck',  default=False,
                help='Enable a checker to skip corrupted files')
optP.add_option('--prestage', action='store_const', const=True, dest='prestage',  default=False,
                help='EXPERIMENTAL : Enable prestager. Make sure that you are authorized')
optP.add_option('--voms', action='store', dest='vomsRoles',  default=None, type='string',
                help="generate proxy with paticular roles. e.g., atlas:/atlas/ca/Role=production,atlas:/atlas/fr/Role=pilot")
optP.add_option('--useNextEvent', action='store_const', const=True, dest='useNextEvent',  default=False,
                help="Set this option if your jobO uses theApp.nextEvent() e.g. for G4")
optP.add_option('--ara', action='store_const', const=True, dest='ara',  default=False,
                help='use Athena ROOT Access')
optP.add_option('--ares', action='store_const', const=True, dest='ares',  default=False,
                help='use Athena ROOT Access + PyAthena, i.e., use athena.py instead of python on WNs')
optP.add_option('--araOutFile', action='store', dest='araOutFile',  default='',
                help='define output files for ARA, e.g., output1.root,output2.root')
optP.add_option('--trf', action='store', dest='trf',  default=False,
                help='run transformation, e.g. --trf "csc_atlfast_trf.py %IN %OUT.AOD.root %OUT.ntuple.root -1 0"')
optP.add_option('--spaceToken', action='store', dest='spaceToken', default='',
                type='string', help='spacetoken for outputs. e.g., ATLASLOCALGROUPDISK')
optP.add_option('--notSkipMissing', action='store_const', const=True, dest='notSkipMissing',  default=False,
                help='If input files are not read from SE, they will be skipped by default. This option disables the functionality')
optP.add_option('--burstSubmit', action='store', dest='burstSubmit', default='',
                type='string', help="Please don't use this option. Only for site validation by experts")
optP.add_option('--devSrv', action='store_const', const=True, dest='devSrv',  default=False,
                help="Please don't use this option. Only for developers to use the dev panda server")
optP.add_option('--useAIDA', action='store_const', const=True, dest='useAIDA',  default=False,
                help="use AIDA")
optP.add_option('--inputType', action='store', dest='inputType', default='',
                type='string', help='File type in input dataset which contains multiple file types')
optP.add_option('--mcData', action='store', dest='mcData', default='',
                type='string', help='Create a symlink with linkName to .dat which is contained in input file')
optP.add_option('--pfnList', action='store', dest='pfnList', default='',
                type='string', help='Name of file which contains a list of input PFNs. Those files can be un-registered in DDM')
optP.add_option('--useExperimental', action='store_const', const=True, dest='useExperimental',  default=False,
                help='use experimental features')
# athena options
optP.add_option('-c',action='store',dest='singleLine',type='string',default='',metavar='COMMAND',
                help='One-liner, runs before any jobOs')
optP.add_option('-p',action='store',dest='preConfig',type='string',default='',metavar='BOOTSTRAP',
                help='location of bootstrap file')
# internal parameters
optP.add_option('--panda_srvURL', action='store', dest='panda_srvURL', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_runConfig', action='store', dest='panda_runConfig', default='',
                type='string', help='internal parameter')
optP.add_option('--panda_srcName', action='store', dest='panda_srcName', default='',
                type='string', help='internal parameter')


# parse options
options,args = optP.parse_args()
if options.verbose:
    print options

# display version
if options.version:
    from pandatools import PandaToolsPkgInfo
    print "Version: %s" % PandaToolsPkgInfo.release_version
    sys.exit(0)

from pandatools import Client
from pandatools import PsubUtils
from pandatools import AthenaUtils
from pandatools import GlobalConfig
from pandatools import PLogger

# update panda-client
if options.update:
    res = PsubUtils.updatePackage(options.verbose)
    if res:
	sys.exit(0)
    else:
	sys.exit(1)

# set grid source file
globalConf = GlobalConfig.getConfig()
if globalConf.grid_src != '' and not os.environ.has_key('PATHENA_GRID_SETUP_SH'):
    os.environ['PATHENA_GRID_SETUP_SH'] = globalConf.grid_src

# get logger
tmpLog = PLogger.getPandaLogger()

# use dev server
if options.devSrv:
    Client.useDevServer()

# set server
if options.panda_srvURL != '':
    Client.setServer(options.panda_srvURL)

# exclude sites
if options.excludedSite != '':
    Client.excludeSite(options.excludedSite)

# site specified
siteSpecified = True
if options.site == 'AUTO':
    siteSpecified = False

# keep original outDS
original_outDS_Name = options.outDS
     
# reset crossSite unless container is used for output 
if not original_outDS_Name.endswith('/'):
    options.crossSite = 0

# error
if options.outDS == '':
    tmpLog.error("no outDS is given\n pathena [--inDS input] --outDS output myJobO.py")
    sys.exit(EC_Config)
if options.split < -1 :
    tmpLog.error("Number of jobs should be a positive integer")
    sys.exit(EC_Config)
if options.shipinput and options.inDS != '' and options.pfnList != '':
    tmpLog.error("--shipInput, --pfnList and --inDS cannot be used at the same time")
    sys.exit(EC_Config)

# libDS
libds_file = '%s/libds_pathena.dat' % os.environ['PANDA_CONFIG_ROOT']
if options.libDS == 'LAST':
    if not os.path.exists(libds_file):
        tmpLog.error("LAST cannot be used until you submit at least one job without --libDS")
        sys.exit(EC_Config)
    # read line
    tmpFile = open(libds_file)
    tmpLibDS = tmpFile.readline()
    tmpFile.close()
    # remove \n
    tmpLibDS = tmpLibDS.replace('\n','')
    # set
    options.libDS = tmpLibDS

# absolute path for PFN list
if options.pfnList != '':
    options.pfnList = os.path.realpath(options.pfnList)

# burst submission
if options.burstSubmit != '':
    # don't scan LRC/LFC
    options.skipScan = True
    # reset cloud/site. They will be overwritten at submission
    options.cloud = None
    options.site  = None
    # disable individual output
    options.individualOutDS = False
    # check libDS stuff
    if options.libDS != '' or options.nobuild:
        tmpLog.error("--libDS or --nobuild cannot be used together with --burstSubmit")
        sys.exit(EC_Config)
        

# split options are mutually exclusive
if (options.nFilesPerJob > 0 and options.nEventsPerJob > 0):
    tmpLog.error("split by files and split by events can not be defined sumaltaneously")
    sys.exit(EC_Config)

# check DBRelease
if options.dbRelease != '' and options.dbRelease.find(':') == -1:
    tmpLog.error("invalid argument for --dbRelease. Must be DatasetName:FileName")  
    sys.exit(EC_Config)
    
# additinal files
options.extFile = options.extFile.split(',')
options.extOutFile = options.extOutFile.split(',')
try:
    options.extOutFile.remove('')
except:
    pass

# set ara on when ares is used
if options.ares:
    options.ara = True

# output files for ARA
if options.ara and options.araOutFile == '':
    tmpLog.error("--araOutFile is needed when ARA (--ara) is used")
    sys.exit(EC_Config)
for tmpName in options.araOutFile.split(','):
    if tmpName != '':
        options.extOutFile.append(tmpName)

# shadow DS for individualOutDS
if options.individualOutDS:
    # use log DS as shadow DS
    suffixShadow = "_log" + suffixShadow

# file list
tmpList = options.filelist.split(',')
options.filelist = []
for tmpItem in tmpList:
    if tmpItem == '':
        continue
    # wild card
    tmpItem = tmpItem.replace('*','.*')
    # append
    options.filelist.append(tmpItem) 
# read file list from file
if options.inputFileList != '':
    rFile = open(options.inputFileList)
    for line in rFile:
        line = re.sub('\n','',line)
        options.filelist.append(line)
    rFile.close()

# removed files
if options.removeFileList == '':
    # empty
    options.removeFileList = []
else:
    # read from file
    rList = []
    rFile = open(options.removeFileList)
    for line in rFile:
        line = re.sub('\n','',line)        
        rList.append(line)
    rFile.close()
    options.removeFileList = rList

# file type
options.inputType = options.inputType.split(',')
try:
    options.inputType.remove('')
except:
    pass

# suppressed streams
options.supStream = options.supStream.upper().split(',')
try:
    options.supStream.remove('')
except:
    pass

# set nFilesPerJob for MC data
if options.mcData != '':
    options.nFilesPerJob = 1
    
# set nfiles
if options.nFilesPerJob > 0 and options.nfiles == 0 and options.split > 0:
    options.nfiles = options.nFilesPerJob * options.split

# check grid-proxy
gridPassPhrase,vomsFQAN = PsubUtils.checkGridProxy('',False,options.verbose,options.vomsRoles)

# set cloud according to country FQAN
expCloudFlag = False
if options.cloud == None and options.burstSubmit == '':
    options.cloud = PsubUtils.getCloudUsingFQAN(defaultCloud,options.verbose)
elif options.cloud != None:
    # use cloud explicitly
    expCloudFlag = True

# correct site
if options.site != 'AUTO':
    origSite = options.site
    # patch for BNL
    if options.site in ['BNL',"ANALY_BNL"]:
        options.site = "ANALY_BNL_ATLAS_1"
    # try to convert DQ2ID to PandaID
    pID = PsubUtils.convertDQ2toPandaID(options.site)
    if pID != '':
        options.site = pID
    # add ANALY
    if not options.site.startswith('ANALY_'):
        options.site = 'ANALY_%s' % options.site
    # check
    if not Client.PandaSites.has_key(options.site):
        tmpLog.error("unknown siteID:%s" % origSite)
        sys.exit(EC_Config)
    # set cloud
    options.cloud = Client.PandaSites[options.site]['cloud']

# check cloud
foundCloud = False
for tmpID,spec in Client.PandaSites.iteritems():
    if options.cloud == spec['cloud']:
        foundCloud = True
        break
if not foundCloud:
    tmpLog.error("unsupported cloud:%s" % options.cloud)
    sys.exit(EC_Config)


# get DN
distinguishedName = PsubUtils.getDN()
if distinguishedName == '':
    sys.exit(EC_Config)

# check outDS format
if not PsubUtils.checkOutDsName(options.outDS,distinguishedName,options.official):
    tmpLog.error("invalid output datasetname:%s" % options.outDS)
    sys.exit(EC_Config)

# save current dir
currentDir = os.path.realpath(os.getcwd())

# get Athena versions
stA,retA = AthenaUtils.getAthenaVer()
# failed
if not stA:
    sys.exit(EC_CMT)
workArea  = retA['workArea'] 
athenaVer = retA['athenaVer'] 
groupArea = retA['groupArea'] 
cacheVer  = retA['cacheVer'] 
nightVer  = retA['nightVer']

# error    
if athenaVer == '':
    tmpStr = ''
    for line in lines:
        tmpStr += ('\n'+line)
    tmpLog.error("could not get Athena version from "+tmpStr)
    sys.exit(EC_CMT)
        
# get run directory
# remove special characters                    
sString=re.sub('[\+]','.',workArea)
runDir = re.sub('^%s' % sString, '', currentDir)
if runDir == currentDir:
    tmpLog.error("you need to run pathena in a directory under %s" % workArea)
    sys.exit(EC_Config)
elif runDir == '':
    runDir = '.'
elif runDir.startswith('/'):
    runDir = runDir[1:]
runDir = runDir+'/'

# get job options
jobO = ''
if options.trf:
    # use trf's parameters
    jobO = options.trf
else:
    # get jobOs from command-line
    if options.preConfig != '':
        jobO += '-p %s ' % options.preConfig
    if options.singleLine != '':
        options.singleLine = options.singleLine.replace('"','\'')
        jobO += '-c "%s" ' % options.singleLine
    for arg in args:
        jobO += ' %s' % arg
if jobO == "":
    tmpLog.error("no jobOptions is given\n   pathena [--inDS input] --outDS output myJobO.py")
    sys.exit(EC_Config)

# ARA uses trf I/F
if options.ara:
    if options.ares:
        jobO = "athena.py " + jobO        
    elif jobO.endswith(".C"):
        jobO = "root -l " + jobO
    else:
        jobO = "python " + jobO        
    options.trf = jobO


if options.panda_runConfig == '':
    # extract run configuration    
    tmpLog.info('extracting run configuration')
    # run ConfigExtractor for normal jobO 
    ret,runConfig = AthenaUtils.extractRunConfig(jobO,options.supStream,options.useAIDA,options.shipinput,options.trf)
else:
    # load from file
    ret = True
    tmpRunConfFile = open(options.panda_runConfig)
    runConfig = pickle.load(tmpRunConfFile)
    tmpRunConfFile.close()
if not options.trf:
    # extractor failed
    if not ret:
        sys.exit(EC_Extractor)
    # shipped files
    if runConfig.other.inputFiles:
        for fileName in runConfig.other.inputFiles:
            # append .root for tag files
            if runConfig.other.inColl:
                match = re.search('\.root(\.\d+)*$',fileName)
                if match == None:
                    fileName = '%s.root' % fileName
            # check ship files in the current dir
            if not os.path.exists(fileName):
                tmpLog.error("%s needs exist in the current directory when --shipInput is used" % fileName)
                sys.exit(EC_Extractor)
            # append to extFile
            options.extFile.append(fileName)
            if not runConfig.input.shipFiles:
                runConfig.input['shipFiles'] = []
            runConfig.input['shipFiles'].append(fileName)
    # generator files
    if runConfig.other.rndmGenFile:
        # append to extFile
        for fileName in runConfig.other.rndmGenFile:
            options.extFile.append(fileName)
    # Condition file
    if runConfig.other.condInput:
        # append to extFile
        for fileName in runConfig.other.condInput:
            if options.addPoolFC == "":
                options.addPoolFC = fileName
            else:
                options.addPoolFC += ",%s" % fileName
else:
    # parse parameters for trf
    oneOut = False
    # replace ; for job sequence
    tmpString = re.sub(';',' ',jobO)
    # look for %OUT
    for tmpItem in tmpString.split():
        match = re.search('\%OUT\.(.+)',tmpItem)
        if match:
            # append basenames to extOutFile
            tmpOutName = match.group(1)
            if not tmpOutName in options.extOutFile:
                options.extOutFile.append(tmpOutName)
                oneOut = True
    # warning if no output
    if not oneOut:
        if not options.ara:
            tmpLog.warning("argument for --trf doesn't contain any %OUT")

# no output jobs
if runConfig.output == {} and options.extOutFile == []:
    tmpLog.error("No output is defined in jobOs or --trf or --extOutFile")
    sys.exit(EC_Extractor)

# check ship files in the current dir
if not runConfig.input.shipFiles:
    runConfig.input.shipFiles = []
for file in runConfig.input.shipFiles:
    if not os.path.exists(file):
        tmpLog.error("%s needs exist in the current directory when using --shipInput" % file)
        sys.exit(EC_Extractor)

# get random number
runConfig.other['rndmNumbers'] = []
if not runConfig.other.rndmStream:
    runConfig.other.rndmStream = []
if len(runConfig.other.rndmStream) != 0:
    if options.norandom:
        print
        print "Initial random seeds need to be defined."
        print "Enter two numbers for each random stream."
        print "  e.g., PYTHIA : 4789899 989240512"
        print
    for stream in runConfig.other.rndmStream:
        if options.norandom:
            # enter manually
            while True:
                str = raw_input("%s : " % stream)
                num = str.split()
                if len(num) == 2:
                    break
                print " Two numbers are needed"
            runConfig.other.rndmNumbers.append([int(num[0]),int(num[1])])
        else:
            # automatic
            runConfig.other.rndmNumbers.append([random.randint(1,5000000),random.randint(1,5000000)])
    if options.norandom:
        print
    
# create tmp dir
if options.tmpDir == '':
    tmpDir = '%s/%s' % (currentDir,commands.getoutput('uuidgen'))
else:
    tmpDir = '%s/%s' % (options.tmpDir,commands.getoutput('uuidgen'))    
os.makedirs(tmpDir)

# set tmp dir in Client
Client.setGlobalTmpDir(tmpDir)

# exit action
def _onExit(dir):
    commands.getoutput('rm -rf %s' % dir)
atexit.register(_onExit,tmpDir)


#####################################################################
# archive sources and send it to HTTP-reachable location

if options.panda_srcName != '':
    # reuse src
    tmpLog.info('reuse source files')
    archiveName = options.panda_srcName
    # go to tmp dir
    os.chdir(tmpDir)
else:
    # copy some athena specific files
    AthenaUtils.copyAthenaStuff(currentDir)

    # matching for extFiles
    def matchExtFile(fileName):
        # .py/.dat/.C/.xml
        for tmpExtention in ['.py','.dat','.C','.xml']:
            if fileName.endswith(tmpExtention):
                return True
        # check filename
        baseName = fileName.split('/')[-1]
        for patt in options.extFile:
            if patt.find('*') == -1:
                # regular matching
                if patt == baseName:
                    return True
            else:
                # use regex for *
                tmpPatt = patt.replace('*','.*')
                if re.search(tmpPatt,baseName) != None:
                    return True
        # not matched
        return False


    archiveName = ""
    if options.libDS == '' and not options.nobuild:
        # archive sources
        tmpLog.info('archiving source files')

        #####################################################################
        # subroutines

        # scan InstallArea to get a list of local packages
        def getFileList(dir,files,forPackage,readLink=True):
            try:
                list = os.listdir(dir)
            except:
                return
            for item in list:
                # skip if doc
                if item == 'doc':
                    continue
                fullName=dir+'/'+item
                if os.path.isdir(fullName):
                    # ignore symlinked dir just under InstallArea/include
                    # they are created for g77
                    if os.path.islink(fullName) and re.search('/InstallArea/include$',dir) != None:
                        pass
                    elif os.path.islink(fullName) and readLink and forPackage:
                        # resolve symlink
                        getFileList(os.readlink(fullName),files,forPackage,readLink)
                    else:
                        getFileList(fullName,files,forPackage,readLink)
                else:
                    if os.path.islink(fullName):
                        if readLink:
                            appFileName = os.readlink(fullName)
                        else:
                            appFileName = os.path.abspath(fullName)                        
                    else:
                        appFileName = os.path.abspath(fullName)
                    # remove redundant //
                    appFilename = re.sub('//','/',appFileName)
                    # append
                    files.append(appFileName)

        # get package list
        def getPackages(_workArea):
            installFiles = []
            getFileList(_workArea+'/InstallArea',installFiles,True)
            # get list of packages
            cmt_config = os.environ['CMTCONFIG']
            _packages = []
            for iFile in installFiles:
                # ignore InstallArea stuff
                if re.search('/InstallArea/',iFile):
                    continue
                # converted to real path
                file = os.path.realpath(iFile)
                # remove special characters
                sString=re.sub('[\+]','.',os.path.realpath(_workArea))
                # look for /share/ , /python/, /i686-slc3-gcc323-opt/, .h
                for target in ('share/','python/',cmt_config+'/','[^/]+\.h'):
                    res = re.search(sString+'/(.+)/'+target,file)
                    if res:
                        # append
                        pName = res.group(1)
                        if target in ['[^/]+\.h']:
                            # convert PackageDir/PackageName/PackageName to PackageDir/PackageName
                            pName = re.sub('/[^/]+$','',pName)
                        if not pName in _packages:
                            if os.path.isdir(_workArea+'/'+pName):
                                _packages.append(pName)
                        break
            return _packages


        # archive files
        def archiveFiles(_workArea,_packages,_archiveFullName):
            _curdir = os.getcwd()
            # change dir
            os.chdir(_workArea)
            for pack in _packages:
                # archive subdirs
                list = os.listdir(pack)
                for item in list:
                    # ignore libraries
                    if item.startswith('i686') or item.startswith('i386') or item.startswith('x86_64') \
                       or item=='dict' or item=='pool' or item =='pool_plugins':
                        continue
                    # run dir
                    if item=='run':
                        files = []
                        getFileList('%s/%s/run' % (_workArea,pack),files,False)
                        for iFile in files:
                            # converted to real path
                            file = os.path.realpath(iFile)
                            # archive .py/.dat/.C files only
                            if matchExtFile(file):
                                # remove special characters                    
                                sString=re.sub('[\+]','.',os.path.realpath(_workArea))
                                relPath = re.sub('^%s/' % sString, '', file)
                                # if replace is failed or the file is symlink, try non-converted path names
                                if relPath.startswith('/') or os.path.islink(iFile):
                                    sString=re.sub('[\+]','.',workArea)
                                    relPath = re.sub(sString+'/','',iFile)
                                if os.path.islink(iFile):
                                    out = commands.getoutput('tar -rh %s -f %s' % (relPath,_archiveFullName))                
                                else:
                                    out = commands.getoutput('tar rf %s %s' % (_archiveFullName,relPath))                
                                if options.verbose:
                                    print relPath
                                    if out != '':    
                                        print out
                        continue
                    # else
                    out = commands.getoutput('tar rf %s %s/%s' % (_archiveFullName,pack,item))
                    if options.verbose:
                        print "%s/%s" % (pack,item)
                        if out != '':    
                            print out
            # back to previous dir
            os.chdir(_curdir)

        #####################################################################
        # execute

        # get packages in private area 
        packages = getPackages(workArea)
        # check TestRelease since it doesn't create any links in InstallArea
        if os.path.exists('%s/TestRelease' % workArea):
            # the TestRelease could be created by hand
            packages.append('TestRelease')

        if options.verbose:
            tmpLog.debug("== private packages ==")
            for pack in packages:
                print pack
            tmpLog.debug("== private files ==")

        # create archive
        archiveName     = 'sources.%s.tar' % commands.getoutput('uuidgen')
        archiveFullName = "%s/%s" % (tmpDir,archiveName)
        # archive private area
        archiveFiles(workArea,packages,archiveFullName)
        # archive current (run) dir
        files = []
        os.chdir(workArea)
        getFileList('%s/%s' % (workArea,runDir),files,False,False)
        for file in files:
            # remove special characters                    
            sString=re.sub('[\+]','.',os.path.realpath(workArea))
            relPath = re.sub(sString+'/','',os.path.realpath(file))
            # if replace is failed or the file is symlink, try non-converted path names
            if relPath.startswith('/') or os.path.islink(file):
                sString=re.sub('[\+]','.',workArea)
                relPath = re.sub(sString+'/','',file)
            # archive .py/.dat/.C/.xml files only
            if not matchExtFile(relPath):
                continue
            # ignore InstallArea
            if relPath.startswith('InstallArea'):
                continue
            # check if already archived
            alreadyFlag = False
            for pack in packages:
                if relPath.startswith(pack):
                    alreadyFlag = True
                    break
            # archive
            if not alreadyFlag:
                if os.path.islink(file):
                    out = commands.getoutput('tar -rh %s -f %s' % (relPath,archiveFullName))                
                else:
                    out = commands.getoutput('tar rf %s %s' % (archiveFullName,relPath))                
                if options.verbose:
                    print relPath
                    if out != '':    
                        print out
        # back to current dir            
        os.chdir(currentDir)

    else:
        # archive jobO
        tmpLog.info('archiving jobOs and modules')

        # get real jobOs
        def getJobOs(dir,files):
            list = os.listdir(dir)
            for item in list:
                fullName=dir+'/'+item
                if os.path.isdir(fullName):
                    # dir
                    getJobOs(fullName,files)
                else:
                    # python and other extFiles
                    if matchExtFile(fullName):
                        files.append(fullName)

        # get jobOs
        files = []
        os.chdir(workArea)
        getJobOs('%s' % workArea,files)
        # create archive
        archiveName     = 'jobO.%s.tar' % commands.getoutput('uuidgen')
        archiveFullName = "%s/%s" % (tmpDir,archiveName)
        # archive
        if options.verbose:
            tmpLog.debug("== py files ==")
        for file in files:
            # remove special characters                    
            sString=re.sub('[\+]','.',workArea)
            relPath = re.sub(sString+'/','',file)
            # append
            out = commands.getoutput('tar -rh %s -f %s' % (relPath,archiveFullName))
            if options.verbose:
                print relPath
                if out != '':    
                    print out


    # archive InstallArea
    if options.libDS == '':
        tmpLog.info('archiving InstallArea')

        # get file list
        def getFiles(dir,files,ignoreLib,ignoreSymLink):
            if options.verbose:
                tmpLog.debug("  getFiles(%s)" % dir)
            try:    
                list = os.listdir(dir)
            except:
                return
            for item in list:
                if ignoreLib and (item.startswith('i686') or item.startswith('i386') or
                                  item.startswith('x86_64')):
                    continue
                fullName=dir+'/'+item
                if os.path.isdir(fullName):
                    # ignore symlinked dir just under InstallArea/include
                    if ignoreSymLink and os.path.islink(fullName) and re.search('InstallArea/include$',dir) != None:
                        continue
                    # dir
                    getFiles(fullName,files,False,ignoreSymLink)
                else:
                    files.append(fullName)

        # get cmt files
        def getCMTFiles(dir,files):
            list = os.listdir(dir)
            for item in list:
                fullName=dir+'/'+item
                if os.path.isdir(fullName):
                    # dir
                    getCMTFiles(fullName,files)
                else:
                    if re.search('cmt/requirements$',fullName) != None:
                        files.append(fullName)

        # get files
        areaList = []
        # workArea must be first
        areaList.append(workArea)
        if groupArea != '':
            areaList.append(groupArea)
        # groupArea archive    
        groupFileName = re.sub('^sources','groupArea',archiveName)
        groupFullName = "%s/%s" % (tmpDir,groupFileName)
        allFiles = []    
        for areaName in areaList:    
            # archive
            if options.verbose:
                tmpLog.debug("== InstallArea under %s ==" % areaName)
            files = []
            cmtFiles = []
            os.chdir(areaName)
            if areaName==workArea and not options.nobuild:
                # ignore i686 for workArea
                getFiles('InstallArea',files,True,True)
            else:
                # groupArea
                if not os.path.exists('InstallArea'):
                    if options.verbose:
                        print "  Doesn't exist. Skip"
                    continue
                getFiles('InstallArea',files,False,False)
                # cmt/requirements is needed for non-release packages
                for itemDir in os.listdir(areaName):
                    if itemDir != 'InstallArea' and os.path.isdir(itemDir) and \
                           (not os.path.islink(itemDir)):
                        getCMTFiles(itemDir,cmtFiles)
            # remove special characters                    
            sString=re.sub('[\+]','.',os.path.realpath(areaName))
            # archive files if they are under the area
            for file in files+cmtFiles:
                relPath = re.sub(sString+'/','',os.path.realpath(file))
                if not relPath.startswith('/'):
                    # use files in private InstallArea instead of group InstallArea
                    if not file in allFiles:
                        # append
                        if file in files:
                            out = commands.getoutput('tar -rh %s -f %s' % (file,archiveFullName))
                        else:
                            # requirements files
                            out = commands.getoutput('tar -rh %s -f %s' % (file,groupFullName))
                        allFiles.append(file)
                        if options.verbose:
                            print file
                            if out != '':    
                                print out
        # append groupArea to sources
        if groupArea != '':
            os.chdir(tmpDir)
            if os.path.exists(groupFileName):
                out = commands.getoutput('tar -rh %s -f %s' % (groupFileName,archiveFullName))
                if out != '':    
                    print out
                commands.getoutput('rm -rf %s' % groupFullName)    


    # back to tmp dir        
    os.chdir(tmpDir)

    # remove some athena specific files
    AthenaUtils.deleteAthenaStuff(currentDir)

    # compress
    status,out = commands.getstatusoutput('gzip %s' % archiveName)
    archiveName += '.gz'
    if status != 0 or options.verbose:
        print out

    # check archive
    status,out = commands.getstatusoutput('ls -l %s' % archiveName)
    if status != 0:
        print out
        tmpLog.error("Failed to archive working area.\n        If you see 'Disk quota exceeded', try '--tmpDir /tmp'") 
        sys.exit(EC_Archive)

    # check symlinks
    tmpLog.info("checking symbolic links")
    status,out = commands.getstatusoutput('tar tvfz %s' % archiveName)
    if status != 0:
        tmpLog.error("Failed to expand archive")
        sys.exit(EC_Archive)
    symlinks = []    
    for line in out.split('\n'):
        items = line.split()
        if items[0].startswith('l') and items[-1].startswith('/'):
            symlinks.append(line)
    if symlinks != []:
        tmpStr  = "Found some unresolved symlinks which may cause a problem\n"
        tmpStr += "     See, e.g., http://savannah.cern.ch/bugs/?43885\n"
        tmpStr += "   Please ignore if you believe they are harmless"
        tmpLog.warning(tmpStr)
        for symlink in symlinks:
            print "  %s" % symlink

    # put sources/jobO via HTTP POST
    if not options.nosubmit:
        tmpLog.info("uploading source/jobO files")
        status,out = Client.putFile(archiveName,options.verbose)
        if out != 'True':
            print out
            tmpLog.error("Failed with %s" % status)
            sys.exit(EC_Post)


####################################################################3
# datasets 

# check if output dataset is unique
outputDSexist = False
outputContExist = False
tmpDatasets = Client.getDatasets(options.outDS,options.verbose)
if len(tmpDatasets) != 0:
    if original_outDS_Name.endswith('/'):
        outputContExist = True
    else:
        outputDSexist = True

# check if shadow dataset exists
shadowDSexist = False
tmpDatasets = Client.getDatasets("%s%s" % (options.outDS,suffixShadow),options.verbose)
if len(tmpDatasets) != 0:
    shadowDSexist = True

# set location when outDS or libDS already exists
if outputDSexist:
    if options.verbose:
        tmpLog.debug("get locations for outDS:%s" % options.outDS)
    outDSlocations = Client.getLocations(options.outDS,[],options.cloud,True,options.verbose)
    if outDSlocations == []:
        tmpLog.error("cannot find locations for existing output dataset:%s" % options.outDS)
        sys.exit(EC_Dataset)
    # convert DQ2ID to Panda siteID
    fFlag = False
    for outDSlocation in outDSlocations:
        convID = PsubUtils.convertDQ2toPandaID(outDSlocation)
        if convID != '':
            options.site  = convID
            options.cloud = Client.PandaSites[convID]['cloud']
            fFlag = True
            tmpLog.info("set site:%s cloud:%s because outDS:%s already exists at %s" % \
                        (options.site,options.cloud,options.outDS,outDSlocations))                
            break
    # not found
    if not fFlag:
        tmpLog.error("cannot find supported sites for existing output datasete:%s" % options.outDS)
        sys.exit(EC_Dataset)
elif options.libDS != '':
    if options.verbose:
        tmpLog.debug("get locations for libDS:%s" % options.libDS)
    libDSlocations = Client.getLocations(options.libDS,[],options.cloud,True,options.verbose)
    if libDSlocations == []:
        tmpLog.error("cannot find locations for existing lib dataset:%s" % options.libDS)
        sys.exit(EC_Dataset)
    # convert DQ2ID to Panda siteID
    fFlag = False
    for libDSlocation in libDSlocations:
        convID = PsubUtils.convertDQ2toPandaID(libDSlocation)
        if convID != '':
            options.site  = convID
            options.cloud = Client.PandaSites[convID]['cloud']
            fFlag = True
            tmpLog.info("set site:%s cloud:%s because libDS:%s exists at %s" % \
                        (options.site,options.cloud,options.libDS,libDSlocations))                
            break
    # not found
    if not fFlag:
        tmpLog.error("cannot find supported sites for existing lib datasete:%s" % options.libDS)
        sys.exit(EC_Dataset)
        

# input datasets    
if options.inDS != '' or options.shipinput or options.pfnList != '':
    if options.inDS != '':
        # query files in shadow dataset
        shadowList = []
        if shadowDSexist:
            shadowList = Client.queryFilesInDataset("%s%s" % (options.outDS,suffixShadow),options.verbose)
        elif outputContExist:
            shadowList = Client.getFilesInShadowDataset(options.outDS,suffixShadow,options.verbose)
        # query files in dataset
        tmpLog.info("query files in %s" % options.inDS)
        inputFileMap  = Client.queryFilesInDataset(options.inDS,options.verbose)
        # remove files
        for tmpKey in inputFileMap.keys():
            if tmpKey in options.removeFileList:
                del inputFileMap[tmpKey]
        # remove log, and check matching
        for fileName in inputFileMap.keys():
            # ignore log file
            if len(re.findall('.log.tgz.\d+$',fileName)) or len(re.findall('.log.tgz$',fileName)):
                del inputFileMap[fileName]
                continue
            # check type
            if options.inputType != []:
                matchType = False
                for tmpType in options.inputType:
                    if tmpType in fileName:
                        matchType = True
                        break
                if not matchType:
                    del inputFileMap[fileName]                    
                    continue
            # filename matching        
            if options.filelist != []:
                # check matching    
                matchFlag = False
                for pattern in options.filelist:
                    if re.search('\*',pattern) != None:
                        # wildcard matching
			if re.search(pattern,fileName) != None:
			    matchFlag = True
			    break
                    else:
                        # normal matching
                        if pattern == fileName:
                            matchFlag =True
                            break
                # doesn't match
                if not matchFlag:
                    del inputFileMap[fileName]
        # get locations when site==AUTO
        if options.site == "AUTO":
            dsLocationMap = Client.getLocations(options.inDS,inputFileMap,options.cloud,False,options.verbose,expCloud=expCloudFlag)
	    # no location
            if dsLocationMap == {}:
                if expCloudFlag:
                    tmpLog.error("could not find supported locations in the %s cloud for %s" % (options.cloud,options.inDS))
                else:
                    tmpLog.error("could not find supported locations for %s" % options.inDS)
                sys.exit(EC_Dataset)
            # run brorage
            tmpSites = []
            for tmpItem in dsLocationMap.values():
                tmpSites += tmpItem
            status,out = Client.runBrokerage(tmpSites,'Atlas-%s' % athenaVer,verbose=options.verbose)
            if status != 0:
                tmpLog.error('failed to run brokerage for automatic assignment: %s' % out)
                sys.exit(EC_Config)
	    if not Client.PandaSites.has_key(out):
                tmpLog.error('brokerage gave wrong PandaSiteID:%s' % out)
                sys.exit(EC_Config)
	    # set site/cloud
            options.site  = out
            options.cloud = Client.PandaSites[options.site]['cloud']
            # destination
            if options.destSE == '':
                options.destSE = options.site
            if options.verbose:
                tmpLog.debug("chosen site=%s destSE=%s" % (options.site,options.destSE))
        # scan local replica catalog
        dsLocation = Client.PandaSites[options.site]['ddm']
        if options.skipScan:
            # skip remote scan
            missList = []
        else:
	    tmpLog.info("scanning LFC for %s" % options.site)
            if Client.getLRC(dsLocation) != None:
                # LRC
                missList = Client.getMissLFNsFromLRC(inputFileMap,Client.getLRC(dsLocation),options.verbose,
                                                     options.nfiles+options.nSkipFiles)
            elif Client.getLFC(dsLocation) != None:
                # LFC
                missList = Client.getMissLFNsFromLFC(inputFileMap,options.site,True,options.verbose,
                                                     options.nfiles+options.nSkipFiles)
            else:
                missList = []
            # choose min missList
            if options.verbose:
                tmpLog.debug("%s holds %s files" % (dsLocation,len(inputFileMap)-len(missList)))
        # remove missing
        for fileName in inputFileMap.keys():
            # missing at the site
            if fileName in missList:
                del inputFileMap[fileName]                    
                continue
        # No files available
        if len(inputFileMap) == len(missList):
            tmpLog.error("No files available at %s" % options.site)
            sys.exit(EC_Dataset)
        # check with shadow
        if len(inputFileMap) == len(shadowList):
            tmpStr  = "all input files had already been used in %s\n" % options.inDS
            tmpStr += "  pathena runs on files which were failed or were not used in\n"
            tmpStr += "  previous submissions if it runs with the same inDS and outDS"
            tmpLog.error(tmpStr)
            sys.exit(EC_Dataset)
        # remove shadow
        for fileName in shadowList:
            del inputFileMap[fileName]
        # make list
        inputFileList = inputFileMap.keys()
        inputFileList.sort()
    elif options.pfnList != '':
        # read PFNs from a file
        rFile = open(options.pfnList)
        for line in rFile:
            line = re.sub('\n','',line)
            inputFileList.append(line)
            inputFileList.sort()
        rFile.close()
    else:
        # ship input files
        devidedByGUID = False 
        # extract GUIDs
        guidCollMap,guidCollList = AthenaUtils.getGUIDfromColl(athenaVer,runConfig.input.shipFiles,
                                                               currentDir,
                                                               options.collRefName,
                                                               options.verbose)
        # if works
        if guidCollList != []:
            # use GUIDs for looping
            inputFileList = guidCollList
            # use GUID boundaries
            devidedByGUID = True
        else:
            # use input collections for looping
            inputFileList = runConfig.input.shipFiles
    # skip files
    if options.nSkipFiles > len(inputFileList):
        tmpStr  = "the number of files in %s is less than nSkipFiles\n" % options.inDS
        tmpStr += " N of files=%s : nSkipFiles=%s" % (len(inputFileList),options.nSkipFiles)
        tmpLog.error(tmpStr)
        sys.exit(EC_Dataset)
    inputFileList = inputFileList[options.nSkipFiles:]
    # use limited number of files
    if options.nfiles > 0:
        inputFileList = inputFileList[:options.nfiles]

    # set # of events for shipInput
    if options.shipinput and options.nFilesPerJob == -1 and options.nEventsPerJob == -1:
	# non GUID boundaries
	if not devidedByGUID:
	    options.nEventsPerJob = 2000

    # set # of split
    #@some interesting default behaviour here
    #@ If number of jobs is "undefined" or "not specified by user"
    #@ split dataset in number of chunsk with nFilesPerJob files per job
    #@ if nFilesPerJob are not specified by user then split AOD files list in chunks of 10 files
    #@ split other file types in chunks of 20 files.
    #
    if options.nEventsPerJob != -1:
        if options.nEventsPerFile == 0:
            options.nEventsPerFile = Client.nEvents(options.inDS,options.verbose,(not options.shipinput),inputFileList,currentDir)
	if options.nEventsPerJob > options.nEventsPerFile:
	    tmpDiv,tmpMod = divmod(options.nEventsPerJob,options.nEventsPerFile)
	    options.nFilesPerJob = tmpDiv
	    if tmpMod != 0:
		options.nFilesPerJob += 1
	    options.nEventsPerJob = -1

	    
    if options.split == -1:
        # count total size for inputs
        totalSize = 0 
        for fileName in inputFileList:
            vals = inputFileMap[fileName]
            try:
                totalSize += long(vals['fsize'])
            except:
                pass
        #@ If number of jobs is not defined then....
        #@ For splitting by files case
        if(options.nEventsPerJob == -1):
            if options.nFilesPerJob > 0:
                defaultNFile = options.nFilesPerJob
            else:
                defaultNFile = 20
            tmpNSplit,tmpMod = divmod(len(inputFileList),defaultNFile)
            if tmpMod != 0:
                tmpNSplit += 1
            # check size limit
            if totalSize/tmpNSplit > maxTotalSize:
                # reset to meet the size limit
                tmpNSplit,tmpMod = divmod(totalSize,maxTotalSize)
                if tmpMod != 0:
                    tmpNSplit += 1
                # calculate N files
                divF,modF = divmod(len(inputFileList),tmpNSplit)
		# reset tmpNSplit
                tmpNSplit,tmpMod = divmod(len(inputFileList),divF)
                if tmpMod != 0:
                    tmpNSplit += 1
                # check again just in case
                iDiv = 0
                subTotal = 0
                for fileName in inputFileList:
                    vals = inputFileMap[fileName]
                    try:
                        subTotal += long(vals['fsize'])
                    except:
                        pass
                    iDiv += 1
                    if iDiv >= divF:
                        # check
                        if subTotal > maxTotalSize:
                            # recalcurate
                            if divF != 1:
                                divF -= 1
                            tmpNSplit,tmpMod = divmod(len(inputFileList),divF)
                            if tmpMod != 0:
                                tmpNSplit += 1
                            break
			# reset
                        iDiv = 0
                        subTotal = 0
            # set            
            options.split = tmpNSplit
        #@ For splitting by events case
        else:
            #@ split by number of events defined
            defaultNFile=1 #Each job has one input file in this case
            #@ tmpNSplit - number of jobs per file in case of splitting by event number
            tmpNSplit, tmpMod = divmod(options.nEventsPerFile, options.nEventsPerJob)
            if tmpMod != 0:
                tmpNSplit +=1
            #@ Number of Jobs calculated here:
            options.split = tmpNSplit*len(inputFileList)

    # input stream for minimum bias
    if runConfig.input.inMinBias or (options.trf and jobO.find('%MININ') != -1):
        if options.minDS == "":
            # read from stdin   
            print
            print "This job uses Minimum-Bias stream"
            while True:
                minbiasDataset = raw_input("Enter dataset name for Minimum Bias : ")
                minbiasDataset = minbiasDataset.strip()
                if minbiasDataset != "":
                    break
        else:
            minbiasDataset = options.minDS
        # query files in dataset
        tmpLog.info("query files in %s" % minbiasDataset)
        tmpList = Client.queryFilesInDataset(minbiasDataset,options.verbose)
        for item in tmpList.keys():
            # remove log
            if re.search('log\.tgz(\.\d+)*',item) != None:
                continue
            minbiasList.append((item,tmpList[item]))
        # sort
        minbiasList.sort()
        # number of files per one signal
        if options.nMin < 0:
            while True:
                str = raw_input("Enter the number of Minimum-Bias files per one signal file : ")
                try:
                    options.nMin = int(str)
                    break
                except:
                    pass
        # check # of files
        if len(minbiasList) < options.nMin:
            tmpLog.error("%s contains only %s files which is less than %s" % \
                         (minbiasDataset,len(minbiasList),options.nMin))
            sys.exit(EC_Dataset)

    # input stream for cavern
    if runConfig.input.inCavern or (options.trf and jobO.find('%CAVIN') != -1):
        if options.cavDS == "":
            # read from stdin                  
            print
            print "This job uses Cavern stream"
            while True:
                cavernDataset = raw_input("Enter dataset name for Cavern : ")
                cavernDataset = cavernDataset.strip()
                if cavernDataset != "":
                    break
        else:
            cavernDataset = options.cavDS
        # query files in dataset
        tmpLog.info("query files in %s" % cavernDataset)
        tmpList = Client.queryFilesInDataset(cavernDataset,options.verbose)
        for item in tmpList.keys():
            # remove log
            if re.search('log\.tgz(\.\d+)*',item) != None:
                continue
            cavernList.append((item,tmpList[item]))
        # sort
        cavernList.sort()
        # number of files per one signal
        if options.nCav < 0:
            while True:
                str = raw_input("Enter the number of Cavern files per one signal file : ")
                try:
                    options.nCav = int(str)
                    break
                except:
                    pass
        # check # of files
        if len(cavernList) < options.nCav:
            tmpLog.error("%s contains only %s files which is less than %s" % \
                         (cavernDataset,len(cavernList),options.nCav))
            sys.exit(EC_Dataset)
    # input stream for beam halo
    if runConfig.input.inBeamHalo or (options.trf and jobO.find('%BHIN') != -1):
	# use common DS
	if options.useCommonHalo:
	    if options.beamHaloDS == "":
                # read from stdin                  
                print
                print "This job uses BeamHalo stream"
                while True:
                    beamHaloDataset = raw_input("Enter dataset name for BeamHalo : ")
                    beamHaloDataset = beamHaloDataset.strip()
                    if beamHaloDataset != "":
                        break
            # query files in dataset
            tmpLog.info("query files in %s" % beamHaloDataset)
            tmpList = Client.queryFilesInDataset(beamHaloDataset,options.verbose)
            for item in tmpList.keys():
                # remove log
                if re.search('log\.tgz(\.\d+)*',item) != None:
                    continue
                beamHaloList.append((item,tmpList[item]))
            # sort
            beamHaloList.sort()
            # number of files per one sub job
            if options.nBeamHalo < 0:
                while True:
                    str = raw_input("Enter the number of BeamHalo files per one sub job : ")
                    try:
                        options.nBeamHalo = int(str)
                        break
                    except:
                        pass
            # check # of files
            if len(beamHaloList) < options.nBeamHalo:
                tmpLog.error("%s contains only %s files which is less than %s" % \
                             (beamHaloDataset,len(beamHaloList),options.nBeamHalo))
                sys.exit(EC_Dataset)
	else:	
            # get DS for A-side        
            if options.beamHaloADS == "":
                # read from stdin                  
                print
                print "This job uses BeamHalo stream"
                while True:
                    beamHaloAdataset = raw_input("Enter dataset name for BeamHalo A-side : ")
                    beamHaloAdataset = beamHaloAdataset.strip()
                    if beamHaloAdataset != "":
                        break
            # get DS for C-side
            if options.beamHaloCDS == "":
                # read from stdin                  
                while True:
                    beamHaloCdataset = raw_input("Enter dataset name for BeamHalo C-side : ")
                    beamHaloCdataset = beamHaloCdataset.strip()
                    if beamHaloCdataset != "":
                        break
            # query files in dataset
            tmpLog.info("query files in %s" % beamHaloAdataset)
            tmpList = Client.queryFilesInDataset(beamHaloAdataset,options.verbose)
            for item in tmpList.keys():
                # remove log
                if re.search('log\.tgz(\.\d+)*',item) != None:
                    continue
                beamHaloAList.append((item,tmpList[item]))
            tmpLog.info("query files in %s" % beamHaloCdataset)
            tmpList = Client.queryFilesInDataset(beamHaloCdataset,options.verbose)
            for item in tmpList.keys():
                # remove log
                if re.search('log\.tgz(\.\d+)*',item) != None:
                    continue
                beamHaloCList.append((item,tmpList[item]))
            # sort
            beamHaloAList.sort()
            beamHaloCList.sort()
            # number of files per one sub job
            if options.nBeamHaloA < 0:
                while True:
                    str = raw_input("Enter the number of BeamHalo files for A-side per one sub job : ")
                    try:
                        options.nBeamHaloA = int(str)
                        break
                    except:
                        pass
            if options.nBeamHaloC < 0:
                # use default ratio
                options.nBeamHaloC = int(0.02/1.02*options.nBeamHaloA)
            # check # of files
            if len(beamHaloAList) < options.nBeamHaloA:
                tmpLog.error("%s contains only %s files which is less than %s" % \
                             (beamHaloAdataset,len(beamHaloAList),options.nBeamHaloA))
                sys.exit(EC_Dataset)
            if len(beamHaloCList) < options.nBeamHaloC:
                tmpLog.error("%s contains only %s files which is less than %s" % \
                             (beamHaloCdataset,len(beamHaloCList),options.nBeamHaloC))
                sys.exit(EC_Dataset)
    # input stream for beam gas
    if runConfig.input.inBeamGas or (options.trf and jobO.find('%BGIN') != -1):
	# use common DS
	if options.useCommonGas:
            # get BeamGas DS
            if options.beamGasDS == "":
                # read from stdin                  
                print
                print "This job uses BeamGas stream"
                while True:
                    beamGasDataset = raw_input("Enter dataset name for BeamGas : ")
                    beamGasDataset = beamGasDataset.strip()
                    if beamGasDataset != "":
                        break
            # query files in dataset
            tmpLog.error("query files in %s" % beamGasDataset)
            tmpList = Client.queryFilesInDataset(beamGasDataset,options.verbose)
            for item in tmpList.keys():
                # remove log
                if re.search('log\.tgz(\.\d+)*',item) != None:
                    continue
                beamGasList.append((item,tmpList[item]))
            # sort
            beamGasList.sort()
            # number of files per one sub job
            if options.nBeamGas < 0:
                while True:
                    str = raw_input("Enter the number of BeamGas files per one sub job : ")
                    try:
                        options.nBeamGas = int(str)
                        break
                    except:
                        pass
            # check # of files
            if len(beamGasList) < options.nBeamGas:
                tmpLog.error("%s contains only %s files which is less than %s" % \
                             (beamGasDataset,len(beamGasList),options.nBeamGas))
                sys.exit(EC_Dataset)
        else:
            # get DS for H
            if options.beamGasHDS == "":
                # read from stdin                  
                print
                print "This job uses BeamGas stream"
                while True:
                    beamGasHdataset = raw_input("Enter dataset name for BeamGas Hydrogen : ")
                    beamGasHdataset = beamGasHdataset.strip()
                    if beamGasHdataset != "":
                        break
            # get DS for C
            if options.beamGasCDS == "":
                # read from stdin                  
                while True:
                    beamGasCdataset = raw_input("Enter dataset name for BeamGas Carbon : ")
                    beamGasCdataset = beamGasCdataset.strip()
                    if beamGasCdataset != "":
                        break
            # get DS for O
            if options.beamGasODS == "":
                # read from stdin                  
                while True:
                    beamGasOdataset = raw_input("Enter dataset name for BeamGas Oxygen : ")
                    beamGasOdataset = beamGasOdataset.strip()
                    if beamGasOdataset != "":
                        break
            # query files in dataset
            tmpLog.info("query files in %s" % beamGasHdataset)
            tmpList = Client.queryFilesInDataset(beamGasHdataset,options.verbose)
            for item in tmpList.keys():
                # remove log
                if re.search('log\.tgz(\.\d+)*',item) != None:
                    continue
                beamGasHList.append((item,tmpList[item]))
            tmpLog.info("query files in %s" % beamGasCdataset)
            tmpList = Client.queryFilesInDataset(beamGasCdataset,options.verbose)
            for item in tmpList.keys():
                # remove log
                if re.search('log\.tgz(\.\d+)*',item) != None:
                    continue
                beamGasCList.append((item,tmpList[item]))
            tmpLog.info("query files in %s" % beamGasOdataset)
            tmpList = Client.queryFilesInDataset(beamGasOdataset,options.verbose)
            for item in tmpList.keys():
                # remove log
                if re.search('log\.tgz(\.\d+)*',item) != None:
                    continue
                beamGasOList.append((item,tmpList[item]))
            # sort
            beamGasHList.sort()
            beamGasCList.sort()
            beamGasOList.sort()        
            # number of files per one sub job
            if options.nBeamGasH < 0:
                while True:
                    str = raw_input("Enter the number of BeamGas files for Hydrogen per one sub job : ")
                    try:
                        options.nBeamGasH = int(str)
                        break
                    except:
                        pass
            if options.nBeamGasC < 0:
                # use default ratio
                options.nBeamGasC = int(options.nBeamGasH*7/90)
            if options.nBeamGasO < 0:
                # use default ratio
                options.nBeamGasO = int(options.nBeamGasH*3/90)
            # check # of files
            if len(beamGasHList) < options.nBeamGasH:
                tmpLog.error("%s contains only %s files which is less than %s" % \
                             (beamGasHdataset,len(beamGasHList),options.nBeamGasH))
                sys.exit(EC_Dataset)
            if len(beamGasCList) < options.nBeamGasC:
                tmpLog.error("%s contains only %s files which is less than %s" % \
                             (beamGasCdataset,len(beamGasCList),options.nBeamGasC))
                sys.exit(EC_Dataset)
            if len(beamGasOList) < options.nBeamGasO:
                tmpLog.error("%s contains only %s files which is less than %s" % \
                             (beamGasOdataset,len(beamGasOList),options.nBeamGasO))
                sys.exit(EC_Dataset)
else:
    if options.split <= 0:
        options.split = 1

# get DB datasets
dbrFiles  = {}
dbrDsList = []
if options.trf or options.dbRelease != '':
    if options.trf:
        # parse jobO for TRF
        tmpItems = jobO.split()
    else:
        # mimic a trf parameter to reuse following algorithm
        tmpItems = ['%DB='+options.dbRelease]
    # look for DBRelease
    for tmpItem in tmpItems:
        match = re.search('%DB=([^:]+):(.+)$',tmpItem)
        if match:
            tmpDbrDS  = match.group(1)
            tmpDbrLFN = match.group(2)
            # get files in the dataset
            if not tmpDbrDS in dbrDsList:
                tmpLog.info("query files in %s" % tmpDbrDS)
                tmpList = Client.queryFilesInDataset(tmpDbrDS,options.verbose)
                # append
                for tmpLFN,tmpVal in tmpList.iteritems():
                    dbrFiles[tmpLFN] = tmpVal
                dbrDsList.append(tmpDbrDS)
            # check
            if not dbrFiles.has_key(tmpDbrLFN):
                tmpLog.error("%s is not in %s" (tmpDbrLFN,tmpDbrDS))
		sys.exit(EC_Dataset)




# choose site automatically when it is still AUTO
if options.site == "AUTO":
    # get sites belonging to a cloud
    tmpSites = []
    for tmpID,spec in Client.PandaSites.iteritems():
        if spec['cloud']==options.cloud and spec['status']=='online':
            # exclude long,xrootd,local queues
            if Client.isExcudedSite(tmpID):
                continue
            tmpSites.append(tmpID)
    status,out = Client.runBrokerage(tmpSites,'Atlas-%s' % athenaVer,verbose=options.verbose)    
    if status != 0:
        tmpLog.error('failed to run brokerage for automatic assignment: %s' % out)  
        sys.exit(EC_Config)
    if not Client.PandaSites.has_key(out):
	tmpLog.error('brokerage gave wrong PandaSiteID:%s' % out)
	sys.exit(EC_Config)
    # set site
    options.site = out

# long queue
if options.long and not options.site.startswith('ANALY_LONG_'):
    tmpsite = re.sub('ANALY_','ANALY_LONG_',options.site)
    tmpsite = re.sub('_\d+$','',tmpsite)
    # if sitename exists
    if Client.PandaSites.has_key(tmpsite):
        options.site = tmpsite

        
# modify outDS name when container is used for output
if original_outDS_Name.endswith('/'):
    options.outDS = re.sub('/$','.%s' % options.site,options.outDS)
    # check outDS
    tmpDatasets = Client.getDatasets(options.outDS,options.verbose)
    if len(tmpDatasets) != 0:
        outputDSexist = True

# index
indexFiles   = 0
indexCavern  = 0
indexMin     = 0
indexBHalo   = 0
indexBHaloA  = 0
indexBHaloC  = 0
indexBGas    = 0
indexBGasH   = 0
indexBGasC   = 0
indexBGasO   = 0
indexNT      = 0
indexHIST    = 0
indexRDO     = 0
indexESD     = 0
indexAOD     = 0
indexAANT    = 0
indexTAG     = 0
indexTHIST   = 0
indexIROOT   = 0
indexEXT     = 0
indexStream1 = 0
indexStream2 = 0
indexStreamG = 0
indexBS      = 0
indexSelBS   = 0
indexMeta    = 0
indexMS      = 0

# get maximum index
def getIndex(list,pattern):
    maxIndex = 0
    for item in list:
        match = re.match(pattern,item)
        if match != None:
            tmpIndex = int(match.group(1))
            if maxIndex < tmpIndex:
                maxIndex = tmpIndex
    return maxIndex

# increase index
if outputDSexist:
    # query files in dataset from DDM
    tmpLog.info("query files in %s" % options.outDS)
    tmpList = Client.queryFilesInDataset(options.outDS,options.verbose)
    # query files in dataset from Panda
    status,tmpMap = Client.queryLastFilesInDataset([options.outDS],options.verbose)
    for tmpLFN in tmpMap[options.outDS]:
        if not tmpLFN in tmpList:
            tmpList[tmpLFN] = None
    # index
    indexHIST    = getIndex(tmpList,"%s\.hist\._(\d+)\.root" % options.outDS)
    indexRDO     = getIndex(tmpList,"%s\.RDO\._(\d+)\.pool\.root" % options.outDS)    
    indexESD     = getIndex(tmpList,"%s\.ESD\._(\d+)\.pool\.root" % options.outDS)
    indexAOD     = getIndex(tmpList,"%s\.AOD\._(\d+)\.pool\.root" % options.outDS)
    indexTAG     = getIndex(tmpList,"%s\.TAG\._(\d+)\.coll\.root" % options.outDS)
    indexStream1 = getIndex(tmpList,"%s\.Stream1\._(\d+)\.pool\.root" % options.outDS)
    indexStream2 = getIndex(tmpList,"%s\.Stream2\._(\d+)\.pool\.root" % options.outDS)
    indexBS      = getIndex(tmpList,"%s\.BS\._(\d+)\.data" % options.outDS)
    if runConfig.output.outSelBS:
        indexSelBS   = getIndex(tmpList,"%s\.%s\._(\d+)\.data" % (options.outDS,runConfig.output.outSelBS))
    if runConfig.output.outNtuple:
        for sName in runConfig.output.outNtuple:
            tmpIndex = getIndex(tmpList,"%s\.%s\._(\d+)\.root" % (options.outDS,sName))
            if tmpIndex > indexNT:
                indexNT  = tmpIndex
    if runConfig.output.outTHIST:            
        for sName in runConfig.output.outTHIST:
            tmpIndex = getIndex(tmpList,"%s\.%s\._(\d+)\.root" % (options.outDS,sName))
            if tmpIndex > indexTHIST:
                indexTHIST  = tmpIndex
    if runConfig.output.outAANT:            
        for aName,sName in runConfig.output.outAANT:
            tmpIndex = getIndex(tmpList,"%s\.%s\._(\d+)\.root" % (options.outDS,sName))
            if tmpIndex > indexAANT:
                indexAANT  = tmpIndex
    if runConfig.output.outIROOT:            
        for sIndex,sName in enumerate(runConfig.output.outIROOT):
            tmpIndex = getIndex(tmpList,"%s\.iROOT%s\._(\d+)\.%s" % (options.outDS,sIndex,sName))
            if tmpIndex > indexIROOT:
                indexIROOT  = tmpIndex
    if runConfig.output.extOutFile: 
        for sIndex,sName in enumerate(runConfig.output.extOutFile):
            # change * to X and add .tgz
            if sName.find('*') != -1:
                sName = sName.replace('*','XYZ')
                sName = '%s.tgz' % sName
            tmpIndex = getIndex(tmpList,"%s\.EXT%s\._(\d+)\.%s" % (options.outDS,sIndex,sName))
            if tmpIndex > indexEXT:
                indexEXT  = tmpIndex
    if runConfig.output.outStreamG:            
        for sName in runConfig.output.outStreamG:
            tmpIndex = getIndex(tmpList,"%s\.%s\._(\d+)\.pool\.root" % (options.outDS,sName))
            if tmpIndex > indexStreamG:
                indexStreamG = tmpIndex
    if runConfig.output.outMeta:            
        for sName,sAsso in runConfig.output.outMeta:
            iMeta = 0
            if sAsso == 'None':
                tmpIndex = getIndex(tmpList,"%s\.META%s\._(\d+)\.root" % (options.outDS,iMeta))
                iMeta += 1
                if tmpIndex > indexMeta:
                    indexMeta = tmpIndex
    if runConfig.output.outMS:                
        for sName,sAsso in runConfig.output.outMS:
            tmpIndex = getIndex(tmpList,"%s\.%s\._(\d+)\.pool\.root" % (options.outDS,sName))
            if tmpIndex > indexMS:
                indexMS = tmpIndex

if options.verbose:
    print "== parameters =="
    print "Site       : %s" % options.site
    print "Athena     : %s" % athenaVer
    if groupArea != '':
        print "Group Area : %s" % groupArea
    if cacheVer != '':
        print "ProdCache  : %s" % cacheVer[1:]
    if nightVer != '':
        print "Nightly    : %s" % nightVer[1:]        
    print "RunDir     : %s" % runDir
    print "jobO       : %s" % jobO.lstrip()


####################################################################3
# submit jobs

# read jobID
jobDefinitionID = 0
jobid_file = '%s/pjobid.dat' % os.environ['PANDA_CONFIG_ROOT']
if os.path.exists(jobid_file):
    try:
        # read line
        tmpJobIdFile = open(jobid_file)
        tmpID = tmpJobIdFile.readline()
        tmpJobIdFile.close()
        # remove \n
        tmpID = tmpID.replace('\n','')
        # convert to int
        jobDefinitionID = long(tmpID) + 1
    except:
        pass

# look for pandatools package
for path in sys.path:
    if path == '':
        path = curDir
    if os.path.exists(path) and 'pandatools' in os.listdir(path):
        # make symlink for module name.
        os.symlink('%s/pandatools' % path,'taskbuffer')
        break

# append tmpdir to import taskbuffer module
sys.path = [tmpDir]+sys.path
from taskbuffer.JobSpec  import JobSpec
from taskbuffer.FileSpec import FileSpec

jobList = []

# full execution string
fullExecString = PsubUtils.convSysArgv()

# job name
jobName = commands.getoutput('uuidgen')

# build job
if options.nobuild:
    pass
elif options.libDS == '': 
    jobB = JobSpec()
    jobB.jobDefinitionID   = jobDefinitionID
    jobB.jobName           = jobName
    jobB.AtlasRelease      = 'Atlas-%s' % athenaVer
    jobB.homepackage       = 'AnalysisTransforms'+cacheVer+nightVer
    jobB.transformation    = '%s/buildJob-00-00-03' % Client.baseURLSUB
    hostName = commands.getoutput('hostname').split('.')[0]
    jobB.destinationDBlock = 'user%s.%s.%s_%s.lib._%06d' % (time.strftime('%y',time.gmtime()),distinguishedName,
                                                            hostName,random.randint(0,100),jobDefinitionID)
    if options.useExperimental and options.destSE != '':
        # write outputs to destSE
        jobB.destinationSE     = options.destSE
    else:
        jobB.destinationSE     = options.site
    jobB.prodSourceLabel   = 'panda'        
    if options.testMode:    
        jobB.processingType = 'prod_test'
    jobB.assignedPriority  = 2000
    jobB.computingSite = options.site
    jobB.cloud = Client.PandaSites[options.site]['cloud']
    jobB.metadata = fullExecString
    fileBO = FileSpec()
    fileBO.lfn = '%s.lib.tgz' % jobB.destinationDBlock
    fileBO.type = 'output'
    fileBO.dataset = jobB.destinationDBlock
    fileBO.destinationDBlock = jobB.destinationDBlock
    fileBO.destinationSE = jobB.destinationSE
    jobB.addFile(fileBO)
    fileBI = FileSpec()
    fileBI.lfn = archiveName
    fileBI.type = 'input'
    jobB.jobParameters     = '-i %s -o %s' % (fileBI.lfn,fileBO.lfn)
    # source URL
    matchURL = re.search("(http.*://[^/]+)/",Client.baseURLSSL)
    if matchURL != None:
        jobB.jobParameters += " --sourceURL %s " % matchURL.group(1)
    # log    
    file = FileSpec()
    file.lfn  = '%s.log.tgz' % jobB.destinationDBlock
    file.type = 'log'
    file.dataset = jobB.destinationDBlock
    file.destinationDBlock = jobB.destinationDBlock
    file.destinationSE = jobB.destinationSE
    jobB.addFile(file)
    # set space token
    for file in jobB.Files:
        if file.type in ['output','log']:
            if options.spaceToken != '':
                file.destinationDBlockToken = options.spaceToken
            else:
                defaulttoken = Client.PandaSites[options.site]['defaulttoken']
                file.destinationDBlockToken = Client.getDefaultSpaceToken(vomsFQAN,defaulttoken)
    # append
    jobList.append(jobB)
else:
    # query files in lib dataset to reuse libraries
    tmpLog.info("query files in %s" % options.libDS)
    tmpList = Client.queryFilesInDataset(options.libDS,options.verbose)
    tmpFileList = []
    tmpGUIDmap = {}
    for fileName in tmpList.keys():
        # ignore log file
        if len(re.findall('.log.tgz.\d+$',fileName)) or len(re.findall('.log.tgz$',fileName)):
            continue
        tmpFileList.append(fileName)
        tmpGUIDmap[fileName] = tmpList[fileName]['guid'] 
    # incomplete libDS
    if tmpFileList == []:
        # query files in dataset from Panda
	status,tmpMap = Client.queryLastFilesInDataset([options.libDS],options.verbose)
        # look for lib.tgz
	for fileName in tmpMap[options.libDS]:
            # ignore log file
            if len(re.findall('.log.tgz.\d+$',fileName)) or len(re.findall('.log.tgz$',fileName)):
                continue
            tmpFileList.append(fileName)
            tmpGUIDmap[fileName] = None
    # incomplete libDS
    if tmpFileList == []:
        tmpLog.error("lib dataset %s is empty" % options.libDS)
        sys.exit(EC_Dataset)
    # check file list                
    if len(tmpFileList) != 1:
        tmpLog.error("dataset %s contains multiple lib.tgz files : %s" % (options.libDS,tmpFileList))
        sys.exit(EC_Dataset)
    # instantiate FileSpec
    fileBO = FileSpec()
    fileBO.lfn = tmpFileList[0]
    fileBO.GUID = tmpGUIDmap[fileBO.lfn]
    fileBO.dataset = options.libDS
    fileBO.destinationDBlock = options.libDS
    if fileBO.GUID != 'NULL':
        fileBO.status = 'ready'
    
# run athena            

if options.inDS != '':
    tmpLog.info("%s files are missing at %s" % (len(missList),options.site))
    tmpLog.info("use %s files" % len(inputFileList)) 
    
# split job
#@ Loop for jobs here
for iSubJob in range(options.split):
    # terminate condition: no remaining files
    if (options.inDS != '' or options.shipinput or options.pfnList != '') and indexFiles >= len(inputFileList):
        break
    # instantiate sub-job
    jobR = JobSpec()
    jobR.jobDefinitionID   = jobDefinitionID
    jobR.jobName           = jobName
    jobR.AtlasRelease      = 'Atlas-%s' % athenaVer
    jobR.homepackage       = 'AnalysisTransforms'+cacheVer+nightVer
    jobR.transformation    = '%s/runAthena-00-00-11' % Client.baseURLSUB
    if options.inDS == '':
        jobR.prodDBlock        = 'NULL'
    else:
        jobR.prodDBlock        = options.inDS
    jobR.destinationDBlock = options.outDS
    if options.useExperimental and options.destSE != '':
        # write outputs to destSE
        jobR.destinationSE     = options.destSE
    else:
        jobR.destinationSE     = options.site
    jobR.prodSourceLabel   = 'user'        
    if options.testMode:    
        jobR.processingType = 'prod_test'
    jobR.assignedPriority  = 1000
    jobR.cloud             = Client.PandaSites[options.site]['cloud']
    jobR.metadata          = fullExecString    
    # memory
    if options.memory != -1:
        jobR.minRamCount = options.memory
    # CPU count
    if options.maxCpuCount != -1:
        jobR.maxCpuCount = options.maxCpuCount
    jobR.computingSite = options.site
    jobR.cloud = Client.PandaSites[options.site]['cloud']
    # source files
    if not options.nobuild:
        fileS = FileSpec()
        fileS.lfn     = fileBO.lfn
        fileS.GUID    = fileBO.GUID
        fileS.type    = 'input'
        fileS.status  = fileBO.status
        fileS.dataset = fileBO.destinationDBlock
        fileS.dispatchDBlock = fileBO.destinationDBlock
        jobR.addFile(fileS)
    # input files
    inList       = []
    minList      = []
    cavList      = []
    bhaloList    = []
    bgasList     = []
    guidBoundary = []
    if options.inDS != '' or options.shipinput or options.pfnList != '':
        # calculate N files
        (divF,modF) = divmod(len(inputFileList),options.split)
        if modF != 0:
            divF += 1
        # if split by files was specified
        if options.nFilesPerJob > 0:
            divF = options.nFilesPerJob 
        # if split by events was specified then
        if options.nEventsPerJob > 0:
           #@Calculate how many events to skip
           nEventsToSkip = nSkips*options.nEventsPerJob
           # @Increment number of skipped blocks
           nSkips = nSkips + 1
           #Take just one file
           divF = 1
           # @ If splitting of file per event is complete then take the next file
           if nEventsToSkip >= options.nEventsPerFile :
               nEventsToSkip = 0
               nSkips        = 1
               indexFiles   += divF
        # File Selector    
        tmpList = inputFileList[indexFiles:indexFiles+divF]
        if options.inDS != '':
            totalSize = 0
	    for fileName in tmpList:
                vals = inputFileMap[fileName]
                # instantiate  FileSpec
                file = FileSpec()
                file.lfn            = fileName
		file.GUID           = vals['guid']
		file.fsize          = vals['fsize']
		file.md5sum         = vals['md5sum']
                file.dataset        = options.inDS
                file.prodDBlock     = options.inDS
                file.dispatchDBlock = options.inDS
                file.type           = 'input'
                file.status         = 'ready'
                jobR.addFile(file)
                inList.append(fileName)
                try:
                    totalSize += long(file.fsize)
                except:
                    pass
            # size check    
            if totalSize > maxTotalSize:
                tmpStr  = "A subjob has %s input files and requires %sMB of disk space.\n" \
                          % (len(tmpList), (totalSize >> 20))
                tmpStr += "  It must be less than %sMB to avoid overflowing the remote disk.\n" \
                          % (maxTotalSize >> 20)
                tmpStr += "  Please split the job using --nFilesPerJob."
                tmpLog.error(tmpStr)
                sys.exit(EC_Split)
        else:        
	    for fileName in tmpList:
                # use GUID boundaries or not
                if devidedByGUID:
                    # collect GUIDs
                    guidBoundary.append(fileName)
                    # fileName is a GUID in this case 
                    realFileName = guidCollMap[fileName]
                    if not realFileName in inList:
                        inList.append(realFileName)
                else:
                    inList.append(fileName)
        # Minimum Bias
        if runConfig.input.inMinBias:
            if indexMin+options.nMin*divF >= len(minbiasList):
                # re-use files when Minimum-Bias files are not enough   
                indexMin = 0   
            tmpList = minbiasList[indexMin:indexMin+options.nMin*divF]
            indexMin += options.nMin*divF
            for fileName,vals in tmpList:
                # instantiate  FileSpec
                file = FileSpec()
                file.lfn            = fileName
                file.GUID           = vals['guid']
                file.fsize          = vals['fsize']
                file.md5sum         = vals['md5sum']
                file.dataset        = minbiasDataset
                file.prodDBlock     = minbiasDataset
                file.dispatchDBlock = minbiasDataset
                file.type       = 'input'
                file.status     = 'ready'
                jobR.addFile(file)
                minList.append(fileName)
            # Cavern
            if indexCavern+options.nCav*divF >= len(cavernList):
                # re-use files when Cavern files are not enough   
                indexCavern = 0   
            tmpList = cavernList[indexCavern:indexCavern+options.nCav*divF]
            indexCavern += options.nCav*divF
            for fileName,vals in tmpList:
                # instantiate  FileSpec
                file = FileSpec()
                file.lfn            = fileName
                file.GUID           = vals['guid']
                file.fsize          = vals['fsize']
                file.md5sum         = vals['md5sum']
                file.dataset        = cavernDataset
                file.prodDBlock     = cavernDataset
                file.dispatchDBlock = cavernDataset
                file.type           = 'input'
                file.status         = 'ready'
                jobR.addFile(file)
                cavList.append(fileName)
        # BeamHalo
        if runConfig.input.inBeamHalo:
            if options.useCommonHalo:
                # integrated dataset
                if indexBHalo+options.nBeamHalo >= len(beamHaloList):
                    # re-use files when BeamHalo files are not enough   
                    indexBHalo = 0   
                tmpList = beamHaloList[indexBHalo:indexBHalo+options.nBeamHalo]
                indexBHalo += options.nBeamHalo
            else:
                # separate datasets
                if indexBHaloA+options.nBeamHaloA >= len(beamHaloAList):
                    # re-use files when BeamHalo files are not enough   
                    indexBHaloA = 0   
                if indexBHaloC+options.nBeamHaloC >= len(beamHaloCList):
                    # re-use files when BeamHalo files are not enough   
                    indexBHaloC = 0   
                tmpList = beamHaloAList[indexBHaloA:indexBHaloA+options.nBeamHaloA] + \
                          beamHaloCList[indexBHaloC:indexBHaloC+options.nBeamHaloC]
                indexBHaloA += options.nBeamHaloA
                indexBHaloC += options.nBeamHaloC
            tmpIndex = 0
            for fileName,vals in tmpList:
                # instantiate  FileSpec
                file = FileSpec()
                file.lfn            = fileName
                file.GUID           = vals['guid']
                file.fsize          = vals['fsize']
                file.md5sum         = vals['md5sum']
                if options.useCommonHalo:
                    # integrated dataset
                    file.dataset        = beamHaloDataset
                    file.prodDBlock     = beamHaloDataset
                    file.dispatchDBlock = beamHaloDataset
                else:
                    # separate datasets
                    if tmpIndex < options.nBeamHaloA:
                        file.dataset        = beamHaloAdataset
                        file.prodDBlock     = beamHaloAdataset
                        file.dispatchDBlock = beamHaloAdataset
                    else:
                        file.dataset        = beamHaloCdataset
                        file.prodDBlock     = beamHaloCdataset
                        file.dispatchDBlock = beamHaloCdataset
                file.type           = 'input'
                file.status         = 'ready'
                jobR.addFile(file)
                bhaloList.append(fileName)
                tmpIndex += 1
        # BeamGas
        if runConfig.input.inBeamGas:        
            if options.useCommonGas:
                # integrated dataset
                if indexBGas+options.nBeamGas >= len(beamGasList):
                    # re-use files when BeamGas files are not enough   
                    indexBGasO = 0   
                tmpList = beamGasList[indexBGas:indexBGas+options.nBeamGas]
                indexBGas += options.nBeamGas
            else:
                # separate dataset
                if indexBGasH+options.nBeamGasH >= len(beamGasHList):
                    # re-use files when BeamGas files are not enough   
                    indexBGasH = 0   
                if indexBGasC+options.nBeamGasC >= len(beamGasCList):
                    # re-use files when BeamGas files are not enough   
                    indexBGasC = 0   
                if indexBGasO+options.nBeamGasO >= len(beamGasOList):
                    # re-use files when BeamGas files are not enough   
                    indexBGasO = 0   
                tmpList = beamGasHList[indexBGasH:indexBGasH+options.nBeamGasH] + \
                          beamGasCList[indexBGasC:indexBGasC+options.nBeamGasC] + \
                          beamGasOList[indexBGasO:indexBGasO+options.nBeamGasO]
                indexBGasH += options.nBeamGasH
                indexBGasC += options.nBeamGasC
                indexBGasO += options.nBeamGasO
            tmpIndex = 0
            for fileName,vals in tmpList:
                # instantiate  FileSpec
                file = FileSpec()
                file.lfn            = fileName
                file.GUID           = vals['guid']
                file.fsize          = vals['fsize']
                file.md5sum         = vals['md5sum']
                if options.useCommonHalo:
                    # integrated dataset
                    file.dataset        = beamGasDataset
                    file.prodDBlock     = beamGasDataset
                    file.dispatchDBlock = beamGasDataset
                else:
                    # separate datasets
                    if tmpIndex < options.nBeamGasH:
                        file.dataset        = beamGasHdataset
                        file.prodDBlock     = beamGasHdataset
                        file.dispatchDBlock = beamGasHdataset
                    elif tmpIndex < (options.nBeamGasH+options.nBeamGasC):
                        file.dataset        = beamGasCdataset
                        file.prodDBlock     = beamGasCdataset
                        file.dispatchDBlock = beamGasCdataset
                    else:
                        file.dataset        = beamGasOdataset
                        file.prodDBlock     = beamGasOdataset
                        file.dispatchDBlock = beamGasOdataset
                file.type           = 'input'
                file.status         = 'ready'
                jobR.addFile(file)
                bgasList.append(fileName)
                tmpIndex += 1
        #@
        #@ important
        #@ Done with an input files description of the job.
        # Increment pointer (index) to a next block of files
        #@ If split by events is requested Index file is incremented in a different place (above)

        if options.nEventsPerJob < 0:
          indexFiles += divF

            
    # output files
    outMap = {}
    if runConfig.output.outNtuple:
        indexNT += 1
        for sName in runConfig.output.outNtuple:
            file = FileSpec()
            file.lfn  = '%s.%s._%05d.root' % (jobR.destinationDBlock,sName,indexNT)
            file.type = 'output'
            file.dataset = jobR.destinationDBlock
            file.destinationDBlock = jobR.destinationDBlock
            if options.individualOutDS:
                tmpSuffix = '_%s' % sName
                file.dataset += tmpSuffix
                file.destinationDBlock += tmpSuffix
            file.destinationSE = jobR.destinationSE
            jobR.addFile(file)
            if not outMap.has_key('ntuple'):
                outMap['ntuple'] = []
            outMap['ntuple'].append((sName,file.lfn))
    if runConfig.output.outHist:
        indexHIST += 1
        file = FileSpec()
        file.lfn  = '%s.hist._%05d.root' % (jobR.destinationDBlock,indexHIST)
        file.type = 'output'
        file.dataset = jobR.destinationDBlock        
        file.destinationDBlock = jobR.destinationDBlock
        if options.individualOutDS:
            tmpSuffix = '_HIST'
            file.dataset += tmpSuffix
            file.destinationDBlock += tmpSuffix
        file.destinationSE = jobR.destinationSE
        jobR.addFile(file)
        outMap['hist'] = file.lfn
    if runConfig.output.outRDO:
        indexRDO += 1        
        file = FileSpec()
        file.lfn  = '%s.RDO._%05d.pool.root' % (jobR.destinationDBlock,indexRDO)        
        file.type = 'output'
        file.dataset = jobR.destinationDBlock        
        file.destinationDBlock = jobR.destinationDBlock
        if options.individualOutDS:
            tmpSuffix = '_RDO'
            file.dataset += tmpSuffix
            file.destinationDBlock += tmpSuffix
        file.destinationSE = jobR.destinationSE
        jobR.addFile(file)
        outMap['RDO'] = file.lfn
    if runConfig.output.outESD:
        indexESD += 1        
        file = FileSpec()
        file.lfn  = '%s.ESD._%05d.pool.root' % (jobR.destinationDBlock,indexESD)        
        file.type = 'output'
        file.dataset = jobR.destinationDBlock        
        file.destinationDBlock = jobR.destinationDBlock
        if options.individualOutDS:
            tmpSuffix = '_ESD'
            file.dataset += tmpSuffix
            file.destinationDBlock += tmpSuffix
        file.destinationSE = jobR.destinationSE
        jobR.addFile(file)
        outMap['ESD'] = file.lfn
    if runConfig.output.outAOD:
        indexAOD += 1                
        file = FileSpec()
        file.lfn  = '%s.AOD._%05d.pool.root' % (jobR.destinationDBlock,indexAOD)        
        file.type = 'output'
        file.dataset = jobR.destinationDBlock        
        file.destinationDBlock = jobR.destinationDBlock
        if options.individualOutDS:
            tmpSuffix = '_AOD'
            file.dataset += tmpSuffix
            file.destinationDBlock += tmpSuffix
        file.destinationSE = jobR.destinationSE
        jobR.addFile(file)
        outMap['AOD'] = file.lfn
    if runConfig.output.outTAG:
        indexTAG += 1                        
        file = FileSpec()
        file.lfn  = '%s.TAG._%05d.coll.root' % (jobR.destinationDBlock,indexTAG)                
        file.type = 'output'
        file.dataset = jobR.destinationDBlock        
        file.destinationDBlock = jobR.destinationDBlock
        if options.individualOutDS:
            tmpSuffix = '_TAG'
            file.dataset += tmpSuffix
            file.destinationDBlock += tmpSuffix
        file.destinationSE = jobR.destinationSE
        jobR.addFile(file)
        outMap['TAG'] = file.lfn
    if runConfig.output.outAANT:
        indexAANT += 1
        sNameList = []
        for aName,sName in runConfig.output.outAANT:
            file = FileSpec()
            file.lfn  = '%s.%s._%05d.root' % (jobR.destinationDBlock,sName,indexAANT)       
            file.type = 'output'
            file.dataset = jobR.destinationDBlock        
            file.destinationDBlock = jobR.destinationDBlock
            if options.individualOutDS:
                tmpSuffix = '_%s' % sName
                file.dataset += tmpSuffix
                file.destinationDBlock += tmpSuffix
            file.destinationSE = jobR.destinationSE
            if not sName in sNameList:
                sNameList.append(sName)
                jobR.addFile(file)
            if not outMap.has_key('AANT'):
                outMap['AANT'] = []
            outMap['AANT'].append((aName,sName,file.lfn))
    if runConfig.output.outTHIST:
        indexTHIST += 1
        for sName in runConfig.output.outTHIST:
            file = FileSpec()
            file.lfn  = '%s.%s._%05d.root' % (jobR.destinationDBlock,sName,indexTHIST)
            file.type = 'output'
            file.dataset = jobR.destinationDBlock
            file.destinationDBlock = jobR.destinationDBlock
            if options.individualOutDS:
                tmpSuffix = '_%s' % sName
                file.dataset += tmpSuffix
                file.destinationDBlock += tmpSuffix
            file.destinationSE = jobR.destinationSE
            jobR.addFile(file)
            if not outMap.has_key('THIST'):
                outMap['THIST'] = []
            outMap['THIST'].append((sName,file.lfn))
    if runConfig.output.outIROOT:
        indexIROOT += 1
        for sIndex,sName in enumerate(runConfig.output.outIROOT):
            file = FileSpec()
            file.lfn  = '%s.iROOT%s._%05d.%s' % (jobR.destinationDBlock,sIndex,indexIROOT,sName)
            file.type = 'output'
            file.dataset = jobR.destinationDBlock
            file.destinationDBlock = jobR.destinationDBlock
            if options.individualOutDS:
                tmpSuffix = '_iROOT%s' % sIndex
                file.dataset += tmpSuffix
                file.destinationDBlock += tmpSuffix
            file.destinationSE = jobR.destinationSE
            jobR.addFile(file)
            if not outMap.has_key('IROOT'):
                outMap['IROOT'] = []
            outMap['IROOT'].append((sName,file.lfn))
    if options.extOutFile:
        indexEXT += 1
        for sIndex,sName in enumerate(options.extOutFile):
            # change * to X and add .tgz
            origSName = sName
            if sName.find('*') != -1:
                sName = sName.replace('*','XYZ')
                sName = '%s.tgz' % sName
            file = FileSpec()
            file.lfn  = '%s.EXT%s._%05d.%s' % (jobR.destinationDBlock,sIndex,indexEXT,sName)
            file.type = 'output'
            file.dataset = jobR.destinationDBlock
            file.destinationDBlock = jobR.destinationDBlock
            if options.individualOutDS:
                tmpSuffix = '_EXT%s' % sIndex
                file.dataset += tmpSuffix
                file.destinationDBlock += tmpSuffix
            file.destinationSE = jobR.destinationSE
            jobR.addFile(file)
            if not outMap.has_key('IROOT'):
                outMap['IROOT'] = []
            outMap['IROOT'].append((origSName,file.lfn))
    if runConfig.output.outStream1:
        indexStream1 += 1                                        
        file = FileSpec()
        file.lfn  = '%s.Stream1._%05d.pool.root' % (jobR.destinationDBlock,indexStream1)
        file.type = 'output'
        file.dataset = jobR.destinationDBlock        
        file.destinationDBlock = jobR.destinationDBlock
        if options.individualOutDS:
            tmpSuffix = '_Stream1'
            file.dataset += tmpSuffix
            file.destinationDBlock += tmpSuffix
        file.destinationSE = jobR.destinationSE
        jobR.addFile(file)
        outMap['Stream1'] = file.lfn
    if runConfig.output.outStream2:
        indexStream2 += 1                                        
        file = FileSpec()
        file.lfn  = '%s.Stream2._%05d.pool.root' % (jobR.destinationDBlock,indexStream2)
        file.type = 'output'
        file.dataset = jobR.destinationDBlock        
        file.destinationDBlock = jobR.destinationDBlock
        if options.individualOutDS:
            tmpSuffix = '_Stream2'
            file.dataset += tmpSuffix
            file.destinationDBlock += tmpSuffix
        file.destinationSE = jobR.destinationSE
        jobR.addFile(file)
        outMap['Stream2'] = file.lfn
    if runConfig.output.outBS:
        indexBS += 1                                        
        file = FileSpec()
        file.lfn  = '%s.BS._%05d.data' % (jobR.destinationDBlock,indexBS)
        file.type = 'output'
        file.dataset = jobR.destinationDBlock        
        file.destinationDBlock = jobR.destinationDBlock
        if options.individualOutDS:
            tmpSuffix = '_BS'
            file.dataset += tmpSuffix
            file.destinationDBlock += tmpSuffix
        file.destinationSE = jobR.destinationSE
        jobR.addFile(file)
        outMap['BS'] = file.lfn
    if runConfig.output.outSelBS:
        indexSelBS += 1                                        
        file = FileSpec()
        file.lfn  = '%s.%s._%05d.data' % (jobR.destinationDBlock,runConfig.output.outSelBS,indexSelBS)
        file.type = 'output'
        file.dataset = jobR.destinationDBlock        
        file.destinationDBlock = jobR.destinationDBlock
        if options.individualOutDS:
            tmpSuffix = '_SelBS'
            file.dataset += tmpSuffix
            file.destinationDBlock += tmpSuffix
        file.destinationSE = jobR.destinationSE
        jobR.addFile(file)
        if not outMap.has_key('IROOT'):
            outMap['IROOT'] = []
        outMap['IROOT'].append(('%s.*.data' % runConfig.output.outSelBS,file.lfn))
    if runConfig.output.outStreamG:
        indexStreamG += 1
        for sName in runConfig.output.outStreamG:
            file = FileSpec()
            file.lfn  = '%s.%s._%05d.pool.root' % (jobR.destinationDBlock,sName,indexStreamG)
            file.type = 'output'
            file.dataset = jobR.destinationDBlock
            file.destinationDBlock = jobR.destinationDBlock
            if options.individualOutDS:
                tmpSuffix = '_%s' % sName
                file.dataset += tmpSuffix
                file.destinationDBlock += tmpSuffix
            file.destinationSE = jobR.destinationSE
            jobR.addFile(file)
            if not outMap.has_key('StreamG'):
                outMap['StreamG'] = []
            outMap['StreamG'].append((sName,file.lfn))
    if runConfig.output.outMeta:
        iMeta = 0
	indexMeta += 1
        for sName,sAsso in runConfig.output.outMeta:
            foundLFN = ''
            if sAsso == 'None':
                # non-associated metadata
                file = FileSpec()
                file.lfn  = '%s.META%s._%05d.root' % (jobR.destinationDBlock,iMeta,indexMeta)
                file.type = 'output'
                file.dataset = jobR.destinationDBlock
                file.destinationDBlock = jobR.destinationDBlock
                if options.individualOutDS:
                    tmpSuffix = '_META%s' % iMeta
                    file.dataset += tmpSuffix
                    file.destinationDBlock += tmpSuffix
                file.destinationSE = jobR.destinationSE
                jobR.addFile(file)
                iMeta += 1
                foundLFN = file.lfn
            elif outMap.has_key(sAsso):
                # Stream1,2
                foundLFN = outMap[sAsso]
            elif sAsso in ['StreamRDO','StreamESD','StreamAOD']:
                # RDO,ESD,AOD
                stKey = re.sub('^Stream','',sAsso)
                if outMap.has_key(stKey):
                    foundLFN = outMap[stKey]
            else:
                # general stream
                if outMap.has_key('StreamG'):
                    for tmpStName,tmpLFN in outMap['StreamG']:
                        if tmpStName == sAsso:
                            foundLFN = tmpLFN
            if foundLFN != '':
                if not outMap.has_key('Meta'):
                    outMap['Meta'] = []
                outMap['Meta'].append((sName,foundLFN))
    if runConfig.output.outMS:
	indexMS += 1
        for sName,sAsso in runConfig.output.outMS:
            file = FileSpec()
            file.lfn  = '%s.%s._%05d.pool.root' % (jobR.destinationDBlock,sName,indexMS)
            file.type = 'output'
            file.dataset = jobR.destinationDBlock
            file.destinationDBlock = jobR.destinationDBlock
            if options.individualOutDS:
                tmpSuffix = '_%s' % sName
                file.dataset += tmpSuffix
                file.destinationDBlock += tmpSuffix
            file.destinationSE = jobR.destinationSE
            jobR.addFile(file)
            if not outMap.has_key('IROOT'):
                outMap['IROOT'] = []
            outMap['IROOT'].append((sAsso,file.lfn))
                                            
    # log
    file = FileSpec()
    file.lfn  = '%s._$PANDAID.log.tgz' % jobR.destinationDBlock
    file.type = 'log'
    file.dataset = jobR.destinationDBlock    
    file.destinationDBlock = jobR.destinationDBlock
    if options.individualOutDS:
        tmpSuffix = '_log'
        file.dataset += tmpSuffix
        file.destinationDBlock += tmpSuffix
    file.destinationSE = jobR.destinationSE
    jobR.addFile(file)
    # set space token
    for file in jobR.Files:
        if file.type in ['output','log']:
            if options.spaceToken != '':
                file.destinationDBlockToken = options.spaceToken
            else:
                defaulttoken = Client.PandaSites[options.site]['defaulttoken']
                file.destinationDBlockToken = Client.getDefaultSpaceToken(vomsFQAN,defaulttoken)
    # job parameters
    param = ''
    if not options.nobuild:
        param  += '-l %s ' % fileS.lfn
    param += '-r %s ' % runDir
    if not options.trf:
        # set jobO parameter
        param += '-j "%s" ' % urllib.quote(jobO)
        # DBRelease
        if options.dbRelease != '':
            tmpItems = options.dbRelease.split(':')
            tmpDbrDS  = tmpItems[0]
            tmpDbrLFN = tmpItems[1]
            # instantiate  FileSpec
            fileName = tmpDbrLFN
            vals     = dbrFiles[tmpDbrLFN]
            file = FileSpec()
            file.lfn            = fileName
            file.GUID           = vals['guid']
            file.fsize          = vals['fsize']
            file.md5sum         = vals['md5sum']
            file.dataset        = tmpDbrDS
            file.prodDBlock     = tmpDbrDS
            file.dispatchDBlock = tmpDbrDS
            file.type       = 'input'
            file.status     = 'ready'
            jobR.addFile(file)
            # set DBRelease parameter
            param += '--dbrFile %s ' % file.lfn
            if options.dbRunNumber != '':
                param += '--dbrRun %s ' % options.dbRunNumber
    else:
        # replace parameters for TRF
        tmpJobO = jobO
        # output : basenames are in outMap['IROOT'] trough extOutFile
        tmpOutMap = []
        for tmpName,tmpLFN in outMap['IROOT']:
            tmpJobO = tmpJobO.replace('%OUT.' + tmpName,tmpName)
            # set correct name in outMap
            tmpOutMap.append((tmpName,tmpLFN))
        # set output for normal TRF (not for ARA)
        if not options.ara:
            outMap['IROOT'] = tmpOutMap 
        # input
	inPattList = [('%IN',inList),('%MININ',minList),('%CAVIN',cavList),
                      ('%BHIN',bhaloList),('%BGIN',bgasList)]    
	for tmpPatt,tmpInList in inPattList:
            if tmpJobO.find(tmpPatt) != -1 and len(tmpInList) > 0:
                tmpJobO = AthenaUtils.replaceParam(tmpPatt,tmpInList,tmpJobO)
        # DBRelease
        for tmpItem in tmpJobO.split():
            match = re.search('%DB=([^:]+):(.+)$',tmpItem)
            if match:
                tmpDbrDS  = match.group(1)
                tmpDbrLFN = match.group(2)
                # instantiate  FileSpec
                fileName = tmpDbrLFN
                vals     = dbrFiles[tmpDbrLFN]
                file = FileSpec()
                file.lfn            = fileName
                file.GUID           = vals['guid']
                file.fsize          = vals['fsize']
                file.md5sum         = vals['md5sum']
                file.dataset        = tmpDbrDS
                file.prodDBlock     = tmpDbrDS
                file.dispatchDBlock = tmpDbrDS
                file.type       = 'input'
                file.status     = 'ready'
                jobR.addFile(file)
                inList.append(fileName)
                # replace parameters
                tmpJobO = tmpJobO.replace(match.group(0),tmpDbrLFN)
        # random seed
        for tmpItem in tmpJobO.split():
            match = re.search('%RNDM=(.+)$',tmpItem)
            if match:
                tmpRndmNum = int(match.group(1)) + iSubJob
                # replace parameters
                tmpJobO = tmpJobO.replace(match.group(0),'%s' % tmpRndmNum)
        # skipEvent
        if tmpJobO.find('%SKIPEVENTS') != -1:
            tmpJobO = tmpJobO.replace('%SKIPEVENTS','%s' % nEventsToSkip)
        # set jobO parameter
        param += '-j "%s" ' % urllib.quote(tmpJobO)		
    param += '-i "%s" ' % inList
    param += '-m "%s" ' % minList
    param += '-n "%s" ' % cavList
    if bhaloList != []:
        param += '--beamHalo "%s" ' % bhaloList
    if bgasList != []:
        param += '--beamGas "%s" ' % bgasList
    param += '-o "%s" ' % outMap
    if runConfig.input.inColl:
        param += '-c '
    if runConfig.input.inBS:
        param += '-b '
    if runConfig.input.backNavi:
        param += '-e '
    if options.shipinput:
        param += '--shipInput '
        # GUID boundaries
        if devidedByGUID:
            param += '--guidBoundary "%s" ' % guidBoundary
            param += '--collRefName %s ' % runConfig.input.collRefName
    pStr1 = ''    
    if runConfig.other.rndmStream != []:
        pStr1 = "AtRndmGenSvc=Service('AtRndmGenSvc');AtRndmGenSvc.Seeds=["
        for stream in runConfig.other.rndmStream:
            num = runConfig.other.rndmNumbers[runConfig.other.rndmStream.index(stream)]
            pStr1 += "'%s %d %d'," % (stream,num[0]+iSubJob,num[1]+iSubJob)
        pStr1 += "]"

    # @ If split by event option was invoked
    pStr2 = ''
    if options.nEventsPerJob > 0 and (not options.trf):
        # @ Number of events to be processed per job
        param1 = "theApp.EvtMax=%s" % options.nEventsPerJob
        # @ possibly skip events in a file
        if options.noInput:
            pStr2 = param1
        else:
            param2 = "EventSelector.SkipEvents=%s" % nEventsToSkip
            # @ Form a string to add to job parameters
            pStr2 = '%s;%s' % (param1,param2)
    # parameter
    if pStr1 != '' or pStr2 != '':
	if pStr1 == '' or pStr2 == '':
	    param += '-f "%s" ' % (pStr1+pStr2)
	else:
            param += '-f "%s;%s" ' % (pStr1,pStr2)
    # libDS 
    if options.libDS != "" or options.nobuild:
        param += '-a %s ' % archiveName
    # addPoolFC
    if options.addPoolFC != "":
        param += '--addPoolFC %s ' % options.addPoolFC
    # use corruption checker
    if options.corCheck:
        param += '--corCheck '
    # disable to skip missing files
    if options.notSkipMissing:
        param += '--notSkipMissing '
    # given PFN 
    if options.pfnList != '':
        param += '--givenPFN '
    # create symlink for MC data
    if options.mcData != '':
        param += '--mcData %s ' % options.mcData
    # source URL
    matchURL = re.search("(http.*://[^/]+)/",Client.baseURLSSL)
    if matchURL != None:
        param += " --sourceURL %s " % matchURL.group(1)
    # run TRF
    if options.trf:
        param += '--trf '
    # use ARA 
    if options.ara:
        param += '--ara '
    # general input format
    if options.generalInput:
        param += '--generalInput '
    # use theApp.nextEvent
    if options.useNextEvent:
        param += '--useNextEvent '
    # assign    
    jobR.jobParameters = param

    if options.verbose:
        tmpLog.debug(param)

    jobList.append(jobR)

# no submit 
if not options.nosubmit:

    # upload proxy for glexec
    if Client.PandaSites.has_key(options.site):
        # delegation
        delResult = PsubUtils.uploadProxy(options.site,options.myproxy,gridPassPhrase,options.verbose)
        if not delResult:
            tmpLog.error("failed to upload proxy")
            sys.exit(EC_MyProxy)

    # normal/burst submission
    if options.burstSubmit == '':
        # normal submission

        # register output dataset
        if not outputDSexist:
            Client.addDataset(options.outDS,options.verbose)

        # register output dataset container
        if original_outDS_Name.endswith('/'):
            # create
            if not outputContExist:
                Client.createContainer(original_outDS_Name,options.verbose)
            # add dataset
            if not outputDSexist:
                Client.addDatasetsToContainer(original_outDS_Name,[options.outDS],options.verbose)

        # register shadow dataset
        if not outputDSexist:
            Client.addDataset("%s%s" % (options.outDS,suffixShadow),options.verbose)

        # register libDS
        if options.libDS == '' and (not options.nobuild):
            Client.addDataset(jobB.destinationDBlock,options.verbose)

        # submit
        tmpLog.info("submit to %s" % options.site)
        status,out = Client.submitJobs(jobList,options.verbose)
    else:
        print "\n=========="
        # burst submission
        origLibDS = jobB.destinationDBlock
        origOutDS = options.outDS
        prevLibDS = jobB.destinationDBlock
        prevOutDS = options.outDS
        # loop over all sites
        for tmpSite in options.burstSubmit.split(','):
            # cloud
            tmpCloud = Client.PandaSites[tmpSite]['cloud']
            newJobList = []
            newLibDS = '%s.%s' % (origLibDS,tmpSite)
            newOutDS = '%s.%s' % (origOutDS,tmpSite)
            repPatt  = {}
            # use buildJob and one runAthena
            for job in jobList[:2]:
                # set cloud/site
                job.cloud          = tmpCloud
                job.computingSite  = tmpSite
                job.destinationSE  = job.computingSite
                # set destDBlock
                if job.prodSourceLabel == 'panda':
                    job.destinationDBlock = newLibDS
                    # pattern to modify LFN
                    subLibDSPatt = '^'+prevLibDS
                    prevLibDS = job.destinationDBlock
                else:
                    job.destinationDBlock = newOutDS
                    # pattern to modify LFN
                    subOutDSPatt = '^'+prevOutDS
                    prevOutDS = job.destinationDBlock
                # correct files
                for file in job.Files:
                    # process output/log and lib.tgz
                    if file.type == 'input':
                        # skip buildJob
                        if job.prodSourceLabel == 'panda':
                            continue
                        # only lib.tgz
                        if re.search(subLibDSPatt,file.dataset) == None:
                            continue
                        # set datasets 
                        file.dataset           = newLibDS
                        file.prodDBlock        = newLibDS
                        file.dispatchDBlock    = newLibDS
                    else:
                        # set datasets
                        if job.prodSourceLabel == 'panda':
                            file.dataset           = newLibDS
                            file.prodDBlock        = newLibDS                    
                            file.destinationDBlock = newLibDS
                        else:
                            file.dataset           = newOutDS
                            file.prodDBlock        = newOutDS                    
                            file.destinationDBlock = newOutDS
                        file.destinationSE = job.destinationSE
                    # modify LFN
                    oldLFN = file.lfn
                    if job.prodSourceLabel == 'panda' or file.type == 'input':
                        newLFN = re.sub(subLibDSPatt,file.dataset,oldLFN)
                    else:
                        newLFN = re.sub(subOutDSPatt,file.dataset,oldLFN)
                    file.lfn = newLFN
                    # pattern for parameter replacement
                    repPatt[oldLFN] = newLFN
                # modify jobParams
                for oldLFN,newLFN in repPatt.iteritems():
                    job.jobParameters = re.sub(oldLFN,newLFN,job.jobParameters)
                # append
                newJobList.append(job)
            # submit
            tmpLog.info("submit to %s" % tmpSite)
            status,out = Client.submitJobs(newJobList,options.verbose)
            if status==0:
                tmpLog.info(" OK")
            else:
                tmpLog.info(" NG : %s" % status)
            time.sleep(2)
        # don't update DB
        sys.exit(0)


    print '==================='
    outstr   = ''
    buildStr = ''
    runStr   = ''
    for index,o in enumerate(out):
        if o != None:
            if index==0:
                # set JobID
                jobID = o[1]
            if index==0 and options.libDS=='' and (not options.nobuild):
                outstr += "  > build\n"
                outstr += "    PandaID=%s\n" % o[0]
                buildStr = '%s' % o[0]            
            elif (index==1 and options.libDS=='') or \
                 (index==0 and (options.libDS!='' or options.nobuild)):
                outstr += "  > run\n"
                outstr += "    PandaID=%s" % o[0]
                runStr = '%s' % o[0]                        
            elif index+1==len(out):
                outstr += "-%s" % o[0]
                runStr += '-%s' % o[0]                                    
    print ' JobID  : %s' % jobID
    print ' Status : %d' % status
    print outstr

    # create dir for DB
    dbdir = os.path.expanduser(os.environ['PANDA_CONFIG_ROOT'])
    if not os.path.exists(dbdir):
        os.makedirs(dbdir)

    # record jobID
    tmpJobIdFile = open(jobid_file,'w')
    tmpJobIdFile.write(str(jobID))
    tmpJobIdFile.close()

    # record libDS
    if options.libDS == '' and not options.nobuild:
        tmpFile = open(libds_file,'w')
        tmpFile.write(jobB.destinationDBlock)
        tmpFile.close()

# go back to current dir
os.chdir(currentDir)

# try another site if input files remain
options.crossSite -= 1
if options.crossSite > 0 and options.inDS != '' and not siteSpecified:
    if missList != []:
        anotherTry = True
        # nfiles
        if options.nfiles != 0:
            if options.nfiles > len(inputFileMap):
                fullExecString = re.sub('--nFiles\s*=*\d+',
                                        '--nFiles=%s' % (options.nfiles-len(inputFileMap)),
                                        fullExecString)
            else:
                anotherTry = False
 	# decrement crossSite counter
        fullExecString = re.sub(' --crossSite\s*=*\d+','',fullExecString)
        fullExecString += ' --crossSite=%s' % options.crossSite
        # don't reuse site
        match = re.search('--excludedSite\s*=*\S+',fullExecString)
        if match != None:
            # append site
            fullExecString = re.sub(match.group(0),'%s,%s' % (match.group(0),options.site),fullExecString)
        else:
            # add option
            fullExecString += ' --excludedSite=%s' % options.site
        # remove --fileList
        fullExecString = re.sub('--fileList\s*=*\S+','',fullExecString)
        # set list of input files
        inputTmpfile = '%s/intmp.%s' % (tmpDir,commands.getoutput('uuidgen'))
        iFile = open(inputTmpfile,'w')
        for tmpMiss in missList:
            iFile.write(tmpMiss+'\n')
        iFile.close()
        fullExecString = re.sub(' --inputFileList\s*=*\S+','',fullExecString)
        fullExecString += ' --inputFileList=%s' % inputTmpfile
        # source name
        if not '--panda_srcName' in fullExecString:
            fullExecString += ' --panda_srcName=%s' % archiveName
        # server URL
        if not '--panda_srvURL' in fullExecString:
            fullExecString += ' --panda_srvURL=%s,%s' % (Client.baseURL,Client.baseURLSSL)
        # run config
        confTmpfile = ''
        if not '--panda_runConfig' in fullExecString:
            conTmpfile = '%s/conftmp.%s' % (tmpDir,commands.getoutput('uuidgen'))
            cFile = open(conTmpfile,'w')
            pickle.dump(runConfig,cFile)
            cFile.close()
            fullExecString += ' --panda_runConfig=%s' % conTmpfile

        # run pathena
        if anotherTry:
            tmpLog.info("trying other sites for the missing files")
            com = 'pathena ' + fullExecString
            if options.verbose:
                tmpLog.debug(com)
            status = os.system(com)
            # delete tmp files
            commands.getoutput('\rm -f %s' % inputTmpfile)
            commands.getoutput('\rm -f %s' % conTmpfile)            
            # exit
            sys.exit(status)

# succeeded
sys.exit(0)
